<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[快速上手k8s——minikube最小实现]]></title>
    <url>%2F2019%2F11%2F23%2Fgolang%2Fopen-source-component%2Fk8s-minikube-started%2F</url>
    <content type="text"><![CDATA[最近在研究k8s，就来写一个关于k8s快速上手，并记录采坑的点。需要的前置知识点：docker、k8s的一些基本概念，下面这个可能对你有帮助。https://juejin.im/post/5d1b2a656fb9a07edc0b7058 什么是k8s我们知道，我们可以将项目制作成docker镜像，然后利用docker去部署我们的项目，这样可以解决很多服务器环境所带来的问题；但是容器多了，容器与容器之间就需要访问，之间就需要网络配置等等，从而就有了docker-compose；但是当我们的服务进行升级，或者服务需要进行调度，扩容等等，这个时候就需要一个大管家来管所有的东西；这个大管家就是 - Kubernetes 初学会遇到的问题因为k8s的东西太多了，所以学习成本现在越来越高，好在k8s已经很多教程。我说一下现在学的时候肯定会遇到的大问题： 国内的问题（国内环境很多镜像拉不到） 本地搭建环境（原来搭建k8s需要一些服务器） 电脑环境的问题（windows和mac都有坑点） 最小实现现在我们就来在本机实现一个最小的k8s的实现，给出一个hello-worldk8s提供了minikube，这个东西可以让你本机一台机器就可以搭建起这个环境。拥有和线上一样的命令行操作和模式，但是不需要你再去创建很多虚拟机来搞事情了。超级方便也。https://minikube.sigs.k8s.io/我们就利用这个来实现，下面来说说步骤：我的本机环境： macOS minikube version: v1.5.2 Docker version 18.03.1-ce 安装环境大致步骤：https://minikube.sigs.k8s.io/docs/start/macos/ brew install minikube brew install docker-machine-driver-vmware minikube start --vm-driver=vmware --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers 成功之后： minikube status 查看minikube的状态 minikube ip 查看minikube的ip minikube dashboard 打开dashboard展示k8s的状态 安装坑点 HyperKit最新版本可能存在问题，所以我使用VMware Fusion实现虚拟化的依赖 国内k8s.gcr.io的相关镜像国内拉取不到，使用mirrorgooglecontainers也无法拉取到，所以使用阿里云的仓库 https://github.com/kubernetes/minikube/issues/3860 如果之前已经使用过minikube start命令，建议先minikube delete，并删除rm -rf ~/.minikube/，然后重新start 进行部署首先描述一下部署要做的事情：linkinstar/mini-go:v1.0 是我已经上传到 docker-hub 里面的一个已经做好的最简单的项目，会暴露一个8080端口的web服务；最终的目标，在k8s创建一个pod，pod中运行一个我们的容器，最终我们在外部可以访问到这个服务 首先创建两个文件12345678910111213141516171819202122# deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: mini-go labels: app: mini-gospec: replicas: 1 selector: matchLabels: app: mini-go template: metadata: labels: app: mini-go spec: containers: - name: mini-go image: linkinstar/mini-go:v1.0 ports: - containerPort: 8080 1234567891011121314# service.yamlapiVersion: v1kind: Servicemetadata: name: mini-go-servicespec: selector: app: mini-go type: NodePort ports: - protocol: TCP port: 80 targetPort: 8080 nodePort: 30008 执行 kubectl create -f service.yaml kubectl create -f deploy.yaml 查看执行成功可以再dashboard中查看执行状态 最终访问地址查看到服务是否正常：http://192.168.231.146:30008/其中的ip是通过 minikube ip 命令查看的 服务操作水平伸缩在现实的业务环境中，当用户的访问增多，我们需要扩展我们的应用，也就是水平的去多部署几个容器，有了k8s之后这件事就变得非常的容易了。 修改 deploy.yaml 文件中的 replicas: 2 改成2个 使用命令：kubectl apply -f deploy.yaml 使配置生效 然后我们就可以看到，原来的一个pod变成了两个，而k8s会将我们的请求负载均衡到每个pod中。整个过程可以说是非常的优雅了。 同样的，当我们需要减少服务的数量时也是相同的道理 版本升级对于应用的版本升级也是同样的道理 修改 deploy.yaml 文件中的 image: linkinstar/mini-go:v2.0 改成2.0 使用命令：kubectl apply -f deploy.yaml 使配置生效 版本回退当我们发现发布的服务问题，想要进行版本回退的时候，就可以使用kubectl rollout undo deployments/mini-go进行版本回退，下面是版本回退过程中 总结 使用minikube可以快速让新手感受到k8s到底是如何使用的 环境配置过程中会有很多问题，需要你耐心解决 k8s在服务编排上面除了以上提到的用法以外还有很多牛逼的功能等着你去发现 学习过程中需要保持一个原则，先用着看看 -&gt; 搞清楚架构 -&gt; 尝试各种功能 -&gt; 学习各个模块的实现 -&gt; 最终实践]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速上手terraform —— 阿里云OSS和ECS的创建]]></title>
    <url>%2F2019%2F11%2F10%2Fgolang%2Fopen-source-component%2Fterraform-aliyun-oss-ecs-test%2F</url>
    <content type="text"><![CDATA[最近在研究terraform，采了一圈坑，记录一下。 什么是terraform？terraform 通过代码配置实现物理机等一些资源的分配。简单说就是，写一个配置文件，启动，就能帮你购买一台云的机器，或者说申请到oss的资源，或者是别的什么。具体功能见官网。 https://www.terraform.io/docs/index.html 名词解释：provider你可以把它看做各个厂商对terraform提供的插件，terraform可以调用这些插件从而实现对资源的操作管理。terraform流程：init -&gt; plan -&gt; apply -&gt; destroy对应为：初始化，计划验证，实际应用，销毁 最小demo我将用一个最小的demo来演示它怎么干活的：通过terraform来创建一个阿里云的oss（以下没有利益相关）只是因为阿里云我有账号而已，其他供应商也有。 我的本地环境：macOS 步骤1 下载相关资源下载对应的客户端：https://www.terraform.io/downloads.html下载解压后得到：terraform的客户端，将它复制到 /usr/local/bin 目录下12$ terraform -versionTerraform v0.12.13 验证一下，输出版本正常的话就好了 下载阿里云对应provider：https://releases.hashicorp.com/terraform-provider-alicloud/1.60.0/我下载的版本为：terraform-provider-alicloud_1.60.0_darwin_amd64.zip 放置到一个你喜欢的目录下，我这边为/Users/LinkinStar/Documents/tf-plugin 步骤2 申请阿里云相关资源创建你的阿里云账号，这个不多说了。然后需要创建一个ram用户https://ram.console.aliyun.com/用户管理 -&gt; 新建用户 -&gt; 保存好AccessKey和Secret 注意！这里还需要对用户进行授权，点击授权给到相关权限就行，我给了全部，方便测试 步骤3 编写文件随便创建一个工作目录，然后创建一个tf文件main.tf1234567891011# Configure the Alicloud Providerprovider &quot;alicloud&quot; &#123; access_key = &quot;LTAIaskjfhadsklfhklasdjfhdsakjlfhdask&quot; secret_key = &quot;6GPashfjksladfhdjskafhdsklajfdhaljfhajfl&quot; region = &quot;cn-beijing&quot;&#125;resource &quot;alicloud_oss_bucket&quot; &quot;bucket-acl&quot; &#123; bucket = &quot;bucket-123456654321-acl&quot; acl = &quot;private&quot;&#125; 其中 access_key 和 secret_key 为你刚才申请ram的信息bucket 的名称你随便取一个 步骤4 运行命令init以下命令均在tf文件当前目录下运行1terraform init -plugin-dir=/Users/LinkinStar/Documents/tf-plugin 注意后面的目录是我们刚才下载的provider的目录出现 Terraform has been successfully initialized! 为成功 plan然后使用命令 terraform plan 出现以下信息123456789101112131415 # alicloud_oss_bucket.bucket-acl will be created + resource &quot;alicloud_oss_bucket&quot; &quot;bucket-acl&quot; &#123; + acl = &quot;private&quot; + bucket = &quot;bucket-123456654321-acl&quot; + creation_date = (known after apply) + extranet_endpoint = (known after apply) + force_destroy = false + id = (known after apply) + intranet_endpoint = (known after apply) + location = (known after apply) + owner = (known after apply) + storage_class = &quot;Standard&quot; &#125;Plan: 1 to add, 0 to change, 0 to destroy. apply运行命令 terraform apply 过程中需要输出yes确认出现以下信息就证明成功了：Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 你可以到：https://oss.console.aliyun.com/overview查看OSS是否已经被创建 destroy最后运行命令 terraform destroy 过程中需要输出yes确认刚才创建的OSS资源就会被销毁，你可以再在网上刷新一下查看 以上就是通过terraform进行的资源操作，同样的，你可以修改tf文件进行更多资源的操作 对ECS进行操作1234567891011121314151617181920212223242526272829303132333435# Configure the Alicloud Providerprovider &quot;alicloud&quot; &#123; access_key = &quot;LTAIaskjfhadsklfhklasdjfhdsakjlfhdask&quot; secret_key = &quot;6GPashfjksladfhdjskafhdsklajfdhaljfhajfl&quot; region = &quot;cn-hangzhou&quot;&#125;# Create a new ECS instance for a VPCresource &quot;alicloud_security_group&quot; &quot;group&quot; &#123; name = &quot;tf_test_foo&quot; description = &quot;foo&quot; vpc_id = &quot;$&#123;alicloud_vpc.vpc.id&#125;&quot;&#125;resource &quot;alicloud_vpc&quot; &quot;vpc&quot; &#123; name = &quot;tf_test_foo&quot; cidr_block = &quot;172.16.0.0/12&quot;&#125;resource &quot;alicloud_vswitch&quot; &quot;vswitch&quot; &#123; vpc_id = &quot;$&#123;alicloud_vpc.vpc.id&#125;&quot; cidr_block = &quot;172.16.0.0/21&quot; availability_zone = &quot;cn-hangzhou-i&quot;&#125; resource &quot;alicloud_instance&quot; &quot;instance&quot; &#123; availability_zone = &quot;cn-hangzhou-i&quot; security_groups = &quot;$&#123;alicloud_security_group.group.*.id&#125;&quot; instance_type = &quot;ecs.t6-c2m1.large&quot; system_disk_category = &quot;cloud_efficiency&quot; image_id = &quot;ubuntu_18_04_64_20G_alibase_20190624.vhd&quot; instance_name = &quot;test_foo&quot; vswitch_id = &quot;$&#123;alicloud_vswitch.vswitch.id&#125;&quot; internet_max_bandwidth_out = 0&#125; 需要注意的是你的账户余额必须&gt;100块，否则会提示余额不足，我这边tf里面写的是非常便宜几分钱一个小时的机器，所以问题不大，你要是害怕你可以不测（再说一遍本文没有利益相关） 相关资源总结provider下载地址：https://releases.hashicorp.com/provider相关文档：https://www.terraform.io/docs/providers/index.html 坑点 国内的墙导致直接执行init是不可以的，它会去官方拉取provider但是会报错，必须手动下载provider并指定-plugin-dir，呼吸不到外面的新鲜空气有点难受。 terraform的文档都是英文的并且provider给到的参数选项不全，不知道该填什么。 阿里云创建的ram用户默认没有权限。 阿里云需要100块？！我充10块钱不行吗？我又花不了那么多，弄得我晚饭都吃不起了。 阿里云很多地区是没有便宜的机器卖的，一直提示我机器没有，很难受。 PS: 以上TF文件均用来测试，所以一些变量没有做抽离，实际使用一些变量如key等会抽离成别的文件统一管理，别被我带跑偏了。 总结总的来说使用tf对于资源的申请还是非常方便的，一个文件就可以搞定，可以做到随时使用随时销毁，同时也支持多次apply对资源进行更新和操作。它不仅可以对服务器进行操作，还有很多公有云的资源如dns等进行操作，并且现在provider支持的厂商很多。 原理相关的博客后续有机会补充，希望不会鸽。]]></content>
      <categories>
        <category>terraform</category>
      </categories>
      <tags>
        <tag>terraform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang中神奇的sync.Pool]]></title>
    <url>%2F2019%2F11%2F09%2Fgolang%2Fsource-code%2Fsync-pool-source-code%2F</url>
    <content type="text"><![CDATA[在 golang 中有一个池，它特别神奇，你只要和它有个约定，你要什么它就给什么，你用完了还可以还回去，但是下次拿的时候呢，确不一定是你上次存的那个，这个池就是 sync.Pool 说实话第一次看到这个东西的时候，真的想不到这个东西有啥用啊，为什么要有这个东西呢？等我看完之后，嗯，还有有点用的；等到有一次优化经历的时候，嗯，这个有点意思了。今天我们就来看看这个神奇的 sync.Pool 简单案例首先我们来看看这个 sync.Pool 是如何使用的，其实非常的简单。它一共只有三个方法我们需要知道的：New、Put、Get 12345678910111213141516171819package mainimport ( "fmt" "sync")var strPool = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return "test str" &#125;,&#125;func main() &#123; str := strPool.Get() fmt.Println(str) strPool.Put(str)&#125; 通过New去定义你这个池子里面放的究竟是什么东西，在这个池子里面你只能放一种类型的东西。比如在上面的例子中我就在池子里面放了字符串。 我们随时可以通过Get方法从池子里面获取我们之前在New里面定义类型的数据。 当我们用完了之后可以通过Put方法放回去，或者放别的同类型的数据进去。 目的那么这个池子的目的是什么呢？其实一句话就可以说明白，就是为了复用已经使用过的对象，来达到优化内存使用和回收的目的。说白了，一开始这个池子会初始化一些对象供你使用，如果不够了呢，自己会通过new产生一些，当你放回去了之后这些对象会被别人进行复用，当对象特别大并且使用非常频繁的时候可以大大的减少对象的创建和回收的时间。 来看看doc其实官方文档里面给出了一些小细节让我们一起来看看 https://golang.google.cn/pkg/sync/#Pool A Pool is a set of temporary objects that may be individually saved and retrieved. Any item stored in the Pool may be removed automatically at any time without notification. If the Pool holds the only reference when this happens, the item might be deallocated. A Pool is safe for use by multiple goroutines simultaneously. Pool’s purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector. That is, it makes it easy to build efficient, thread-safe free lists. However, it is not suitable for all free lists. An appropriate use of a Pool is to manage a group of temporary items silently shared among and potentially reused by concurrent independent clients of a package. Pool provides a way to amortize allocation overhead across many clients. An example of good use of a Pool is in the fmt package, which maintains a dynamically-sized store of temporary output buffers. The store scales under load (when many goroutines are actively printing) and shrinks when quiescent. On the other hand, a free list maintained as part of a short-lived object is not a suitable use for a Pool, since the overhead does not amortize well in that scenario. It is more efficient to have such objects implement their own free list. A Pool must not be copied after first use. 注意其中加粗的部分，我列一下其中的点，建议还是尝试去阅读doc里面的说明。 临时对象 自动移除 当这个对象的引用只有sync.Pool持有时，这个对象内存会被释放 多线程安全 目的就是缓存并重用对象，减少GC的压力 自动扩容、缩容 不要去拷贝pool，也就是说最好单例 源码分析下面我们从源码层面来看看这个 sync.Pool；可能需要你有GPM模型和GC的相关知识。使用golang版本： go version go1.13 结构12345678910111213141516171819202122232425262728type Pool struct &#123; noCopy noCopy local unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal localSize uintptr // size of the local array victim unsafe.Pointer // local from previous cycle victimSize uintptr // size of victims array // New optionally specifies a function to generate // a value when Get would otherwise return nil. // It may not be changed concurrently with calls to Get. New func() interface&#123;&#125;&#125;// Local per-P Pool appendix.type poolLocalInternal struct &#123; private interface&#123;&#125; // Can be used only by the respective P. shared poolChain // Local P can pushHead/popHead; any P can popTail.&#125;type poolLocal struct &#123; poolLocalInternal // Prevents false sharing on widespread platforms with // 128 mod (cache line size) = 0 . pad [128 - unsafe.Sizeof(poolLocalInternal&#123;&#125;)%128]byte&#125; 我们可以看到其实结构并不复杂，但是如果自己看的话有点懵。注意几个细节就ok。 local这里面真正的是[P]poolLocal其中P就是GPM模型中的P，有多少个P数组就有多大，也就是每个P维护了一个本地的poolLocal。 poolLocal里面维护了一个private一个shared，看名字其实就很明显了，private是给自己用的，而shared的是一个队列，可以给别人用的。注释写的也很清楚，自己可以从队列的头部存然后从头部取，而别的P可以从尾部取。 victim这个从字面上面也可以知道，幸存者嘛，当进行gc的stw时候，会将local中的对象移到victim中去，也就是说幸存了一次gc， Get123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960func (p *Pool) Get() interface&#123;&#125; &#123; ...... l, pid := p.pin() x := l.private l.private = nil if x == nil &#123; // Try to pop the head of the local shard. We prefer // the head over the tail for temporal locality of // reuse. x, _ = l.shared.popHead() if x == nil &#123; x = p.getSlow(pid) &#125; &#125; runtime_procUnpin() ...... if x == nil &amp;&amp; p.New != nil &#123; x = p.New() &#125; return x&#125;func (p *Pool) getSlow(pid int) interface&#123;&#125; &#123; // See the comment in pin regarding ordering of the loads. size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire locals := p.local // load-consume // Try to steal one element from other procs. for i := 0; i &lt; int(size); i++ &#123; l := indexLocal(locals, (pid+i+1)%int(size)) if x, _ := l.shared.popTail(); x != nil &#123; return x &#125; &#125; // Try the victim cache. We do this after attempting to steal // from all primary caches because we want objects in the // victim cache to age out if at all possible. size = atomic.LoadUintptr(&amp;p.victimSize) if uintptr(pid) &gt;= size &#123; return nil &#125; locals = p.victim l := indexLocal(locals, pid) if x := l.private; x != nil &#123; l.private = nil return x &#125; for i := 0; i &lt; int(size); i++ &#123; l := indexLocal(locals, (pid+i)%int(size)) if x, _ := l.shared.popTail(); x != nil &#123; return x &#125; &#125; // Mark the victim cache as empty for future gets don't bother // with it. atomic.StoreUintptr(&amp;p.victimSize, 0) return nil&#125; 我去掉了其中一些竞态分析的代码，Get的逻辑其实非常清晰。 如果 private 不是空的，那就直接拿来用 如果 private 是空的，那就先去本地的shared队列里面从头 pop 一个 如果本地的 shared 也没有了，那 getSlow 去拿，其实就是去别的P的 shared 里面偷，偷不到回去 victim 幸存者里面找 如果最后都没有，那就只能调用 New 方法创建一个了 我随手画了一下，可能不是特别准确，意思到位了 Put1234567891011121314151617// Put adds x to the pool.func (p *Pool) Put(x interface&#123;&#125;) &#123; if x == nil &#123; return &#125; ...... l, _ := p.pin() if l.private == nil &#123; l.private = x x = nil &#125; if x != nil &#123; l.shared.pushHead(x) &#125; runtime_procUnpin() ......&#125; 看完Get其实Put就很简单了 如果 private 没有，就放在 private 如果 private 有了，那么就放到 shared 队列的头部 实际测试让我们实际写个测试的案例来测测具体使用时会有什么样的变化 Put之后马上Get123456789101112131415var pool = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return "123" &#125;,&#125;func main() &#123; t := pool.Get().(string) fmt.Println(t) pool.Put("321") t2 := pool.Get().(string) fmt.Println(t2)&#125; 输出：123321 Put之后GC后Get123456789101112131415161718192021222324252627var pool = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return "123" &#125;,&#125;func main() &#123; t := pool.Get().(string) fmt.Println(t) pool.Put("321") pool.Put("321") pool.Put("321") pool.Put("321") runtime.GC() time.Sleep(1 * time.Second) t2 := pool.Get().(string) fmt.Println(t2) runtime.GC() time.Sleep(1 * time.Second) t2 = pool.Get().(string) fmt.Println(t2)&#125; 输出：123321123 你知道为什么吗？ 总结这次总结来点不一样的，提几个问题吧。 什么情况下适合使用sync.Pool呢？ sync.Pool的对象什么时候会被回收呢？ sync.Pool是如何实现线程安全的？如果你能回答上面的问题，证明你对它已经足够了解了，那么就可以尝试在具体的情况下使用它来玩玩了。试试吧~]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>sync.Pool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解golang内存分配]]></title>
    <url>%2F2019%2F11%2F02%2Fgolang%2Fbasic%2Fgo-memory%2F</url>
    <content type="text"><![CDATA[我们知道所有程序运行都需要使用内存，而内存的管理和分配又是非常重要的，它决定了你的程序能不能在有限的资源内跑的更快。可以设想一下，如果你自己来设计的一个内存分配的规则，会遇到什么问题呢？如果你有了一大块内存你要怎么去合理的分配和使用呢？今天我们通过几张图来看看golang中的内存分配是怎样的。 前置知识：对golang的GPM模型有所了解，对GC有一定的了解，有助于你理解下面的内容。 想一想我们首先来想一下，如果我们自己来分配内存的时候可能会遇到什么问题。 我想要512G，你能给吗？操作系统的内存不是你想要多少就给你多少的。比如我跟操作系统说我要512G内存，你赶紧给我，不给我我就掐死你，如果你是操作系统，是不是立马就想把我给结束了？ 能随便分割吗？如果我拿到一块内存，挺大的，你把它想象成一块地，我今天要用这块地的这个部分，肯定是从中间切一块出来用，然后明天要另一个部分，然后再切出来一部分。如果随便切，今天要一块三角形，明天要一块圆形，那么肯定会留有很多小块的地方，那些地方没有办法被合理的使用，就会浪费。等到想再要一块正方形的地的时候发现没地方可以切了。 不用了我需要放回去吗？如果我占用了很大一块内存资源，然后用完了，现在不需要了，那自私的人肯定想着，我就偷偷一直占用不行吗？显然是不可以的，不然的话你的应用程序就每天占用着一台机器大量的资源不释放，别的人都没得用了，肯定想把你干掉。所以用完了要放回去。 –其实上面的问题就是内存分配常见的一些问题，那为了高效、合理利用内存，势必需要一些人的管理和帮助，下面我们就来看看那些在golang中的管理者，看看他们是如何帮助我们去管理和分配内存的。 内存的管理者这张图里面就是golang中内存的管理者们，下面我来依次介绍一下 OS首先是操作系统，他拥有着全部的机器内存，我们的程序必须向它要。但是他是大领导，很忙的，你不能没事总找他要，很烦，所以每次都会向他一大块内存（1M打底）他会给你一票地址，但是实际其实并不会直接给你分配内存，但是你用到了自然会有。 heap这个是我们程序中最大的内存持有区域，堆，他管理着那些很大的内存块，同时是他向操作系统去申请内存的，全局只有他一个大人物。他还需要将从操作系统拿过来的内存进行一定的划分，划分成一整块一整块的样子方便管理，同时记录内存的使用情况，方便整理和回收。 central这个是二把手，有很多，他们会负责将内存划分成足够小的单元，同时需要向下提供内存的分配工作，这个时候就需要一些合理的分配措施了，这个我们后面再说。 cache这个是最后一个小领导了，管理着最终线程需要使用的内存资源，而且每个线程都会有一个独立的cache，一对一绑定，这样使用的时候就会直接从对应的cache中去取来使用，这样的好处是不用和别人发生争抢。如果所有的线程都从一个地方进行取用，那么势必会造成你也要用，我也要用的情况。 总结从上面的图我们可以基本明白一个总体的思路是说：需要有人总体去把控所有内存资源的使用，做到统一的调度和管理，这样可以方便后续的回收和利用。同时需要下面有人负责最终使用的分配，从而能达到一个内存的快速分配而不发生争抢。 内存的分配结构我们知道了内存的管理者是谁，那么现在我们再来看看内存到底是怎么划分的，究竟是切成一个个长方形还是切成一个个圆形了呢？ 这张图就表示了整个golang中内存的分配结构长什么样子。 arena这块区域最大，明显就是用来存放我们最终的对象，里面分成了一个个8K大小的房间，每个房间我们称为page。（这里虽然写了它是512G，但是你心里要有B数，你电脑根本没这么大的内存，其实操作系统只是给了你地址而已）同时几个page组合在一起的大房间又叫做mspan（这个是golang中内存管理的基本单元） bitmap然后我们再来看第二大的bitmap，它是用来表示arena中存放的对象的一些信息，包括这个对象GC标志，还有标识这个对象是否包含指针。你肯定就好奇，干嘛要有这个呢？这其实也很好理解，golang在进行垃圾回收的时候是根据引用的可达性分析来确定一个对象是否可以被回收，同时采用的是三色标记法进行标记对象，所以这里需要有bitmap来保存这些信息。（具体如果不清楚垃圾回收的细节可以去看看我之前写的有关垃圾回收的部分） spans最后是spans，这里保存了mspan的指针，这个也好理解，为了方便管理那一个个大房间嘛 内存分配那么最后我们来看看我们创建的一个对象最后究竟会经历些什么，是怎么样分配的呢？首先要说明的是，golang很聪明的，如果一个变量可以分配在栈上，那么就不会被分配在堆上，这样可以有效的节约资源（具体我后续还会写别的来说明golang中的变量）。总之我们这里讨论的是分配在堆上的情况。整个流程差不多类似就是这样，嗯，你只要把内存想象成房间，现在房价那么贵，你懂的 分配流程 大对象： &gt;32KB 的对象，直接从heap上分配 小对象： 16B &lt; obj &lt;= 32KB 计算规格在mcache中找到合适大小的mspan进行分配（你有多大就住多大的房子竟可能的不要浪费房子的空间） 微小对象： &lt;=16B 的对象使用mcache的tiny分配器分配；（如果将多个微小对象组合起来，用单块内存（object）存储，可有效减少内存浪费。） 秉持原则：给到最合适的大小，然后能凑一起的凑一起挤一挤 扩容如果不够怎么办呢？不够肯定就要扩容了呗，当不够的时候就会向领导上报，逐层上报，最终想办法拿到内存。如果cache没有相应规格大小的mspan，则向central申请如果central没有相应规格大小的mspan，则向heap申请如果heap中也没有合适大小的mspan，则向操作系统申请 回收最后还要记得，如果你用完了，不用了，会有后台的清洁工来回收掉，最终还是会还回去的。一方面呢：cache用完了还给central，central就可以给别的cache用；central用完了就会还给heap…最终都不用的还给操作系统 总结至此golang的内存分配也就说的差不多了，其中一些细节可能没有说到，可能你还需要看看别的文章来补一补。总结一下： 你多大人住多大的房间，不多给 划分成合理的大小可以一起给一起回收，大小合适的分割才不会浪费 用完还回去，需要标记怎么样算用完了 每个人线程有独立的缓冲区来进行快速分配，不用抢来抢去]]></content>
      <categories>
        <category>golang基础</category>
      </categories>
      <tags>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话图解gin源码]]></title>
    <url>%2F2019%2F08%2F20%2Fgolang%2Fopen-source-component%2Fgin%2F</url>
    <content type="text"><![CDATA[最近在网上搜了一下，对于gin框架用的人还是比较多的，我自己之前也在使用，但是对于源码解析这块，我没有看到自己想看到的那种从框架入手的解析图，所以嘿嘿嘿，我的机会就来了，今天就带来最完整的gin源码图解。希望通过这篇博客你也能自己学会拆轮子。 PS：本文建立在你已经能熟练使用gin的基础之上，如果还没用过可以去官网看一下：https://gin-gonic.com/zh-cn/docs/然后gin是对golang的http包的封装，所以最好对http包也要有了解。 整体分析逻辑先来说明一下我整体拆解的逻辑，对于一个框架，我喜欢从下面几个方面去入手拆解： 启动方式 如何使用 实现与特点针对于gin，我也将从这几个方面去入手，就会得到下面几个问题，带着问题看源码是必备条件。首先启动的时候gin做了些什么？gin封装了什么然后怎么去实现的？gin整体结构是怎么样的，有哪些结构？… 然后使用一个比较小的demo，然后先从方法入手，进源码看。12345678910111213141516func main() &#123; router := gin.Default() router.Use(gin.Recovery()) router.GET("/test", func(context *gin.Context) &#123; context.JSON(http.StatusOK, gin.H&#123; "code" : 1, "message" : "xxx", &#125;) &#125;) if err := router.Run(); err != nil &#123; panic(err) &#125;&#125; 整体结构认识Engine是一个总的引擎，保存了各个组件的信息RouterGroup是一个路由组，保存了路由信息trees是一棵树，保存了url与handle的映射关系engine中的pool用于复用ContextContext用于request中传递值 这样你就对gin的整个结构有了大致的认识，当然有一些细节字段我这里就不展开了。 每个方法分析gin.Default()调用过程大概是这样，这是一个gin的初始化方法，目的是为了创建整个引擎，并初始化相关参数。初始化RouterGroup、pool等 router.Use()这个是一个使用中间件的方法，当然中间件也有别的方法，这边使用use举例，其实就是将请求过程中需要调用的中间件放入到HandlersChain，这个是一个数组PS：你不知道中间件？那你用一下就知道了，我们很多时候再请求前后需要加入通用方法如鉴权等，就会用到它 router.GET这个就是构建url和具体处理请求的handle的关系了，其实目标很明确，就是要将这组关系存入到最终的trees中去，这里先这样，后面会有详细解释。 router.Run()这个就非常简单了，就是调用golang中http包下的方法监听服务端口而已，没啥好说的，启动了嘛。 func (engine *Engine) ServeHTTP你以为这就完了？？？上面只是启动的时候，那么请求过来的时候怎么办呢？我们知道如果要接收请求，那么就需要实现下面这个接口123type Handler interface &#123; ServeHTTP(ResponseWriter, *Request)&#125; 接口中ServeHTTP就是用来处理请求的，所有请求都会经过这个方法去处理。那么gin是怎么实现这个方法的呢？这个就是调用过程，总的来说就是根据请求的url和method找到对应的handle去处理，还记得之前那棵树吗？对，就是去里面找。同时利用context进行参数传递，最后注意很隐秘的用c.Next进行递归的调用（我已开始都没找到）为啥要递归？因为有中间件鸭！handle是一个链式过程而非只有一个handle 至此所有gin的框架里面的内容应该都包括了，包括整个实现过程，其实并不复杂，你可以根据上面的过程在源码中找到对应的地方详细查看。 好在哪里？我们看源码肯定不能看完，哦，知道怎么实现就完事了，我们最终的目的是要学习其中的优点，那么这个框架有哪些好的地方呢？我个人总结了以下三点供你参考。 context使用context包含了Request，Writer等信息，用它来传递参数，也算是利用golang的一个特性去做的，我没想到。 sync.pool利用pool来重复利用对象，因为请求很多，所以会产生很多数量的context，利用sync.pool进行复用，从而减少内存的分配也提高了效率，值得学习。 trees用什么数据结构存放的url和handle的映射关系呢？我一开始想到的就是map，直接弄个hashmap存一下不就好了嘛，没想到gin用了一颗树来进行存放，内部实现很复杂，这里不做展开，据说是radix tree，我就是把它理解成字典树，从而提高存储和查询，棒。 总结其实gin框架本身实现好像并不是很复杂，很适合新手进行学习，其中也有一些设计思想可以借鉴。]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解goroutine调度]]></title>
    <url>%2F2019%2F08%2F15%2Fgolang%2Fbasic%2Fgoroutine-dis%2F</url>
    <content type="text"><![CDATA[其实这个话题我早就想做了，奈何这个问题确实有点复杂，看了很多文章才有了一点点自己的理解。从golang一开始的使用我就已经开始好奇了，这个goroutine到底是怎么实现的呢？怎么就能搞出一个和线程类似，但是性能又那么好的东西的呢？ 模型三个小伙子在看整体结构之前，我先来介绍三个小伙子，golang为了实现goroutine，定义了这样三个小伙子，让他们帮忙去实现。 G表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 MM代表着真正的执行计算资源。在绑定有效的P后，进入调度器循环；而调度器循环的机制大致是从各种队列、P的本地队列中获取G，切换到G的执行栈上并执行G的函数，调用goexit做清理工作并回到M，如此反复。M并不保留G状态，这是G可以跨M调度的基础。 P表示逻辑processor，P的数量决定了系统内最大可并行的G的数量（前提：系统的物理cpu核数&gt;=P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 整体结构模型我们先来看下面的这张图，从大体结构上看，我们就能理解goroutine了 看到这个图你应该理解了一半，下面我来说明一下。我们知道，在操作系统眼里其实只有cpu和线程，它去控制着各个线程的调度，切换，执行等等，而对于goroutine的实现其实非常类似；在golang层面，首先我们要知道，最终在外面执行的肯定是线程，但是我们内部开出的那些goroutine到哪里去了呢？golang提出GPM模型，在G的眼里，只有P，P保存了需要执行的那些goroutine；而在整个go调度的层面，对外的是M，P会找到一个M，让它去与外面的线程交互，从而去真正执行程序。从这里我们可以发现，其实goroutine的调度器整个就是一个小型的操作系统，内部去造出了类似的部件去完成goroutine的实现，而因为是在内部实现，所以解决了操作系统层面所带来的线程创建慢等问题。 但是，同时这样的调度也会有问题，所以需要一些额外的措施！ 调度中的问题问题1 G不均我们知道，现实情况有的goroutine运行的快，有的慢，那么势必肯定会带来的问题就是，忙的忙死，闲的闲死，go肯定不允许摸鱼的P存在，势必要榨干所有劳动力。如果你没有任务，那么，我们看到模型中还有一个全局G队列的存在，如本地队列已满，会一次性转移半数到全局队列。其他闲的小伙子就会从全局队列中拿；（顺便说一下，优先级是先拿下一个需要执行的，然后去本地队列中拿，再去全局队列中拿，先把自己的做完再做别人的嘛）同时如果全局都没有了，就会去抢别人的做。 问题2 任务卡主了万一有个程序员启动一个goroutine去执行一个任务，然后这个任务一直睡觉（sleep）就是循环睡觉，那咋办嘛！你作为执行人，你总不能说，让它一直占用着一整个线程的资源，然后后面的goroutine都卡主了，那如果只有一个核心P，不就完蛋了？聪明的go才不会那么傻，它采用了抢占式调度来解决这个问题。只要你这个任务执行超过一定的时间（10ms），那么这个任务就会被标识为可抢占的，那么别的goroutine就可以抢先进来执行。只要下次这个goroutine进行函数调用，那么就会被强占，同时也会保护现场，然后重新放入P的本地队列里面等待下次执行。谁来做的呢？sysmon，就是这个背后默默付出的人，它是一个后台运行的监控线程，它来监控那些长时间运行的G任务然后设置可以强占的标识符。（同时顺便提一下，它还会做的一些事情，例如，释放闲置的span内存，2分钟的默认gc等） 问题3 阻塞可怎么办？我们经常使用goroutine还有一个场景就是网络请求和IO操作，这种阻塞的情况下，我们的G和M又会怎么做呢？这个时候有个叫做netpoller的东西出现了，当每次有一个网络请求阻塞的时候，如果按照原来的方法这个时候这个请求会阻塞线程，而有了netpoller这个东西，可以将请求阻塞到goroutine。意思是说，当阻塞出现的时候，当前goroutine会被阻塞，等待阻塞notify，而放出M去做别的g，而当阻塞恢复的时候，netpoller就会通知对应的m可以做原来的g了。同时还要顺便提一句，当P发现没有任务的时候，除了会找本地和全局，也会去netpoll中找。 问题4 系统方法调用阻塞？还有一个问题，我们自己想可能比较难想到，就是当调用一些系统方法的时候，如果系统方法调用的时候发生阻塞就比较麻烦了。下面引用一段话：当G被阻塞在某个系统调用上时，此时G会阻塞在_Gsyscall状态，M也处于block on syscall状态，此时的M可被抢占调度：执行该G的M会与P解绑，而P则尝试与其它idle的M绑定，继续执行其它G。如果没有其它idle的M，但P的Local队列中仍然有G需要执行，则创建一个新的M；当系统调用完成后，G会重新尝试获取一个idle的P进入它的Local队列恢复执行，如果没有idle的P，G会被标记为runnable加入到Global队列。 源码一瞥12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394struct G&#123; uintptr stackguard; // 分段栈的可用空间下界 uintptr stackbase; // 分段栈的栈基址 Gobuf sched; //进程切换时，利用sched域来保存上下文 uintptr stack0; FuncVal* fnstart; // goroutine运行的函数 void* param; // 用于传递参数，睡眠时其它goroutine设置param，唤醒时此goroutine可以获取 int16 status; // 状态Gidle,Grunnable,Grunning,Gsyscall,Gwaiting,Gdead int64 goid; // goroutine的id号 G* schedlink; M* m; // for debuggers, but offset not hard-coded M* lockedm; // G被锁定只能在这个m上运行 uintptr gopc; // 创建这个goroutine的go表达式的pc ...&#125;;struct M&#123; G* g0; // 带有调度栈的goroutine G* gsignal; // signal-handling G 处理信号的goroutine void (*mstartfn)(void); G* curg; // M中当前运行的goroutine P* p; // 关联P以执行Go代码 (如果没有执行Go代码则P为nil) P* nextp; int32 id; int32 mallocing; //状态 int32 throwing; int32 gcing; int32 locks; int32 helpgc; //不为0表示此m在做帮忙gc。helpgc等于n只是一个编号 bool blockingsyscall; bool spinning; Note park; M* alllink; // 这个域用于链接allm M* schedlink; MCache *mcache; G* lockedg; M* nextwaitm; // next M waiting for lock GCStats gcstats; ...&#125;;struct P&#123; Lock; uint32 status; // Pidle或Prunning等 P* link; uint32 schedtick; // 每次调度时将它加一 M* m; // 链接到它关联的M (nil if idle) MCache* mcache; G* runq[256]; int32 runqhead; int32 runqtail; // Available G's (status == Gdead) G* gfree; int32 gfreecnt; byte pad[64];&#125;;struct Sched &#123; Lock; uint64 goidgen; M* midle; // idle m's waiting for work int32 nmidle; // number of idle m's waiting for work int32 nmidlelocked; // number of locked m's waiting for work int3 mcount; // number of m's that have been created int32 maxmcount; // maximum number of m's allowed (or die) P* pidle; // idle P's uint32 npidle; //idle P的数量 uint32 nmspinning; // Global runnable queue. G* runqhead; G* runqtail; int32 runqsize; // Global cache of dead G's. Lock gflock; G* gfree; int32 stopwait; Note stopnote; uint32 sysmonwait; Note sysmonnote; uint64 lastpoll; int32 profilehz; // cpu profiling rate&#125; 总结goroutine的设计总的来说就是参考操作系统的设计，所有的目的很明确就是为了在整个运行过程中能充分利用已有的资源，尽可能在有限的资源里面多做事情，利用gpm的模型以及一些netpoller、sysmon等帮助在阻塞的时候也能合理利用资源，从而达到我们现在高效的goroutine 参考资料：https://tiancaiamao.gitbooks.io/go-internals/content/zh/05.1.htmlhttp://morsmachine.dk/go-schedulerhttp://morsmachine.dk/netpollerhttps://studygolang.com/articles/10116]]></content>
      <categories>
        <category>golang基础</category>
      </categories>
      <tags>
        <tag>goroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的interface]]></title>
    <url>%2F2019%2F08%2F10%2Fgolang%2Fsource-code%2Finterface-source-code%2F</url>
    <content type="text"><![CDATA[由于golang中说interface的文章太多了，很多都已经说的很细节了，所以我再说感觉也有点难。于是总结出几个关键问题，供你参考，如果能做到准确无误有理有据的回答，那么interface应该是没有问题了。 问题 interface底层结构有哪两种，分别是什么样子的，里面保存了哪些信息？ 其中tab是什么时候生成的？ 从别的类型转换成interface，从interface转换成别的类型，这两者的过程是怎么样的？ 两个interface之间是否可以比较？ golang底层是如何判断一个类型是否实现了一个interface？ 1、底层结构12345678910111213141516171819202122232425262728293031type eface struct &#123; // 16 bytes on a 64bit arch _type *_type data unsafe.Pointer&#125;type iface struct &#123; // 16 bytes on a 64bit arch tab *itab data unsafe.Pointer&#125;type itab struct &#123; // 40 bytes on a 64bit arch inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches. _ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter.&#125;type _type struct &#123; // 48 bytes on a 64bit arch size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldalign uint8 kind uint8 alg *typeAlg // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; https://draveness.me/golang/basic/golang-interface.html 2、tabtab结构是itab，里面包含了interfacetype，_type，fun，编译期生成。https://github.com/teh-cmc/go-internals/blob/master/chapter2_interfaces/README.md 3、类型转换由其他类型转换成interface转eface转空接口，很简单，将Eface中的data指针指向原型数据，type指针会指向数据的Type结构体。 转iface与eface相同，但是需要赋值到itab，并且需要做检测，只有实现接口所有方法才可以进行转换。 interface转其他类型那没话说，直接反射走起有这样的语法解决v, ok := i.(T)转换的时候也需要比较能否进行转换 4、类型比较两个interface是可以比较的，http://docs.studygolang.com/ref/spec#Comparison_operators其中说到Interface values are comparable. Two interface values are equal if they have identical dynamic types and equal dynamic values or if both have value nil.只要两个interface的动态类型相同和值相同就可以。 5、判断实现这个判断其实在检测的时候都需要用到。检测就是看Type中的方法表是否包含了InterfaceType的方法表中的所有方法，并把Type方法表中的实现部分拷到Itab的func那张表中。其中表中的数据都是排序过的，所以对比起来快。https://tiancaiamao.gitbooks.io/go-internals/content/zh/07.2.html]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>interface</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang之channel]]></title>
    <url>%2F2019%2F08%2F05%2Fgolang%2Fsource-code%2Fchannel-source-code%2F</url>
    <content type="text"><![CDATA[go中的一个精髓就是就是channel，那么你有没有想过，它究竟是怎么实现的呢？我之前就怀疑过，是不是就是通过一个数组保存了一下传入的数据，然后在接收方读一读就完事了，那么阻塞又是怎么实现的呢？close的时候需要注意些什么呢？ 结构首先我们来看一下channel的结构是怎么样的。1234567891011121314151617181920type hchan struct &#123; qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex&#125; 其实看注释这几个字段都非常好理解，解释一下其中几个：elemtype是表示这个channel中存放的是什么类型的数据；sendx、recvx两个索引指向底层循环数组recvq、sendq两个双向链表保存那些等待的goroutinelock？对就是lock，不然你以为并发的时候channel怎么办？锁呗。 PS: 其实和我一开始想的差不多，底层就是利用一个循环数组来实现的带有缓冲的channel，利用两个index标记的移动来记录发送和读取，然后用一个计数器表示当前还有多少个元素，easy 但是如果你想着go只有这么点东西，那你就太小看它了，细节能把你看哭，嘿嘿嘿，下面来看看源码中具体的接收和发送是怎么实现的。 实现本质：channel发送接收数据的本质是数据拷贝！ 接收我会删除其中一些细节部分，留下其中重要的点看一下，如果希望看到全部，请自行阅读源码。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; // ...... // 如果收一个nil的channel不会panic的，而是被阻塞，gopark就是将当前goroutine阻塞 if c == nil &#123; if !block &#123; return &#125; gopark(nil, nil, "chan receive (nil chan)", traceEvGoStop, 2) throw("unreachable") &#125; // ...... // 加锁哦！防止并操作channel lock(&amp;c.lock) // 处理关闭的情况和无data的情况 if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; if raceenabled &#123; raceacquire(unsafe.Pointer(c)) &#125; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; // 当无缓冲 或者 是有缓冲但是缓冲满了 这两种情况下去recv if sg := c.sendq.dequeue(); sg != nil &#123; recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; // 剩下的情况就是有缓冲的情况，如果有数据的话进if里面，里面其实就是将缓冲中的数据拿出来，并且移动相对应的索引，减少qcount if c.qcount &gt; 0 &#123; // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled &#123; raceacquire(qp) racerelease(qp) &#125; if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- unlock(&amp;c.lock) return true, true &#125; if !block &#123; unlock(&amp;c.lock) return false, false &#125; // 如果当前没有数据，那么只能阻塞咯 // no sender available: block on this channel. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) goparkunlock(&amp;c.lock, "chan receive", traceEvGoBlockRecv, 3) // someone woke us up if mysg != gp.waiting &#123; throw("G waiting list is corrupted") &#125; gp.waiting = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; 发送发送其实和接受异曲同工，也是处理其中几种情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; // 发给一个nil的channel就panic呗 if c == nil &#123; if !block &#123; return false &#125; gopark(nil, nil, "chan send (nil chan)", traceEvGoStop, 2) throw("unreachable") &#125; // ...... // 如果 // 不是缓冲的channel而且没有接受者正在接受 // 是缓冲的channel但是缓冲满了 // 那就直接返回 if !block &amp;&amp; c.closed == 0 &amp;&amp; ((c.dataqsiz == 0 &amp;&amp; c.recvq.first == nil) || (c.dataqsiz &gt; 0 &amp;&amp; c.qcount == c.dataqsiz)) &#123; return false &#125; // ...... // 加锁！ lock(&amp;c.lock) // 如果加锁完了之后发现被关了，要死，直接解锁并panic if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError("send on closed channel")) &#125; // 当有接收者，那就直接发给它就好了 if sg := c.recvq.dequeue(); sg != nil &#123; send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; // 如果是缓冲的，而且还有空间，那么久放到缓冲里面去，移动对应的索引 if c.qcount &lt; c.dataqsiz &#123; // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled &#123; raceacquire(qp) racerelease(qp) &#125; typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ unlock(&amp;c.lock) return true &#125; if !block &#123; unlock(&amp;c.lock) return false &#125; // 当没有缓冲了，那么就需要阻塞发送人了 gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) goparkunlock(&amp;c.lock, "chan send", traceEvGoBlockSend, 3) // someone woke us up. if mysg != gp.waiting &#123; throw("G waiting list is corrupted") &#125; gp.waiting = nil if gp.param == nil &#123; if c.closed == 0 &#123; throw("chansend: spurious wakeup") &#125; panic(plainError("send on closed channel")) &#125; gp.param = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; mysg.c = nil releaseSudog(mysg) return true&#125; 关闭12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576func closechan(c *hchan) &#123; // 关闭一个nil的channel 那就panic if c == nil &#123; panic(plainError("close of nil channel")) &#125; // 关闭也是要加锁的！ lock(&amp;c.lock) if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError("close of closed channel")) &#125; if raceenabled &#123; callerpc := getcallerpc() racewritepc(unsafe.Pointer(c), callerpc, funcPC(closechan)) racerelease(unsafe.Pointer(c)) &#125; // 设置标志 c.closed = 1 var glist *g // 处理所有的接收者，注意即使关闭了，也是可以接收的，因为有缓冲，缓冲里面还有东西 // release all readers for &#123; sg := c.recvq.dequeue() if sg == nil &#123; break &#125; if sg.elem != nil &#123; typedmemclr(c.elemtype, sg.elem) sg.elem = nil &#125; if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil if raceenabled &#123; raceacquireg(gp, unsafe.Pointer(c)) &#125; gp.schedlink.set(glist) glist = gp &#125; // 但是对于发送的来说，如果你关闭了，还有人在发，那么就会无情的panic了，这个在发送的代码里面可以看到，在这里是处理所有发送的goroutine就可以了 // release all writers (they will panic) for &#123; sg := c.sendq.dequeue() if sg == nil &#123; break &#125; sg.elem = nil if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil if raceenabled &#123; raceacquireg(gp, unsafe.Pointer(c)) &#125; gp.schedlink.set(glist) glist = gp &#125; unlock(&amp;c.lock) // Ready all Gs now that we've dropped the channel lock. for glist != nil &#123; gp := glist glist = glist.schedlink.ptr() gp.schedlink = 0 goready(gp, 3) &#125;&#125; 在这里总结一下出现 panic 的情况： close 一个 nil 的 channel close 一个已经 closed 的 channel 向一个 closed 的 channel 发送消息]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你可能不知道的redis]]></title>
    <url>%2F2019%2F07%2F20%2Fmiddleware%2Fredis%2Fredis-knowledge-point%2F</url>
    <content type="text"><![CDATA[以下是针对redis的知识点整理，用于复习，主要以罗列为主，详细具体讲解可以参考书《Redis设计与实现》，你可以过一遍看看有无知识点遗漏。个人能力有限，如果你还有补充可以再下方评论指出，万分感谢。 基础知识点 数据类型string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)底层实现包括：SDS动态字符串，双向链表，数组加链表，渐进式hash，跳表 redis是单线程 redis默认有16个数据库，默认先用0，可以使用select命令切换 过期策略惰性删除：当这个key被访问到，但是已经过期，那就删除定期删除：过一定时间，拿出一定的key判断，进行删除，如果超过一定数量，继续拿出一定的key进行判断删除，时间存在上限 内存超限当redis使用超过内存限制会根据策略来执行：noeviction：不能put，但是可以del和get，默认是这个策略volatile-lru：在有过期时间的key中，淘汰最少用的（redis是近似lru算法，会随机取几个淘汰最少用的）volatile-ttl：在有过期时间的key中，淘汰过期时间最短的（剩下寿命最短的）volatile-random：在有过期时间的key中，随机淘汰allkeys-lru：所有key中，淘汰最少用的allkeys-random：所有key中，随机淘汰 持久化方式RDB、AOF、混合RDB：类似快照，将当前数据拍个照片保存，利用操作系统的 COW 机制来进行数据段页面的分离，进行对数据的复制，同时，新数据会在新的page上面AOF：利用日志来完成记录，当逻辑处理完成记录日志，利用日志重放来恢复，会对aof进行重写瘦身，通过fsync来保证数据刷到磁盘上面，1s一次混合：rdb存储，增量用AOF，重启后先读取rdb再重放aof 通信协议RESP直观的文本协议，利用一些特殊字符来确定当前的是什么语句 redis事务redis可以使用multi/exec/discard。multi 指示事务的开始，exec 指示事务的执行，discard 指示事务的丢弃。redis是不支持回滚的，只能把当前所有的执行丢弃，事务性能不高，使用管道优化，提供watch机制监听改变，但是针对改变只是知道，但是不予处理。 keys和scan通过这两个命令可以找到对应的键，keys会卡，scan不会但是慢一点，可能会重复。 redis对内存优化在存放大key或者数量量大列表时，会对存储结构进行压缩，ziplist当删除多个键的时候内存不会马上被回收，因为操作系统的内存是按页来的，只有这个页上面的key全部删除才能回收，同时，新的key会利用删除后的空间 Pipeline网络交互中利用了内核的特性，客户发送消息时，只需要将消息写入本地缓存，就马上返回，不需要等待，后续操作由内核网关去异步发送，而读取返回消息时也是读取的本地相对应的读缓存，如果没有数据就需要等待网络返回数据从而从缓存中读取数据，这个时候是真正耗时的时候。 常用架构 单机单个节点使用 主从一方面是从做备份，一方面从可以提供get服务Redis同步的是指令流，增量同步，利用buffer进行缓冲，可能出现buffer充满的时候就需要进行快照复制，将主节点进行快照，然后直接发送给从节点，然后从节点删除所有数据，然后从节点进行加载，然后同步新的指令，速度慢。 哨兵利用哨兵去自动选举出主节点，同时出现异常自动重新选出主节点 集群codis：利用中间代理人去访问，将不同的槽位分配到相应的redis节点上面，client通过codis进行访问，codis可以配置多个，通过zk来同步槽位。cluster：官方给出，去中心化，自动维护槽位分布。 应用场景 分布式锁setnx(set if not exists)一开始不支持设置过期时间，后来2.8更新后，命令为：set lock true ex 5 nx但是超时是存在问题的，如果再时间限制过程中，没有执行完，锁自动释放了，但是实际还在执行，但是别的线程可以抢到锁去执行了。 MQ可以利用list来实现消息队列的操作，rpop lpush，redis的list是一个双端队列，同时也支持阻塞拿出消息，来支持队列为空的时候的问题同时有PubSub支持订阅，但是少了很多功能如ack，不能保证数据一定成功发出，后面引入Stream HyperLogLog用于模糊统计，可以用于统计网站的uv（单个用户访问只能算一次），利用矩阵。 GeoHashredis支持存储地理位置，并且进行附近统计和距离计算 布隆过滤rebloom用于最大化效率去重，会有误判，利用hash原理。实际适用于：爬虫系统过滤已经查过的url、垃圾邮件过滤、缓存击穿防御等，总之用于数据量大、要求速度快、可以忍受误判的情况。 问题解决缓存击穿多次查询那些一定不存在的数据，或者当前数据不在缓存的高并发查询，导致巨大流量涌入数据库。解决方式： 缓存空对象，如果可以缓存空对象，将空对象作为null缓存起来，让缓存强制命中（提前缓存或者惰性缓存均可）。存在问题：当空对象多时，浪费了缓存的空间。 利用布隆过滤器缓存出现过的key，保证不在过滤器里面的key一定不存在，布隆过滤器节省很多空间 缓存雪崩情况一：多数类似缓存同时过期，导致对这些key的查询同时落到数据库。情况二：缓存服务器直接挂掉，导致所有请求全部落到数据库，导致后续雪崩。这里的解决方式就需要视情况而定：情况一的话，可以尝试设置缓存过期时间为随机值，不让同类型缓存同时过期。情况二的话，首先优先保证架构上面能压住，尽可能保证有redis的备份节点可以恢复，当然也要做planB，万一全部缓存节点全部挂，最前面网关层面要要做到限流，后续服务需要做降级或熔断，这个时候就不是缓存的问题了，就是架构的问题了。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你可能不知道的mysql]]></title>
    <url>%2F2019%2F07%2F15%2Fmiddleware%2Fmysql%2Fmysql-knowledge-point%2F</url>
    <content type="text"><![CDATA[以下是针对mysql的知识点整理，用于复习，主要以罗列为主，详细具体讲解可以参考书《高性能mysql》，你可以过一遍看看有无知识点遗漏。 执行sql过程客户端 -&gt; 连接器 -&gt; 分析器 -&gt; 优化器 -&gt; 执行器 -&gt; 存储引擎连接器：连接上数据库，长连接分析器：分析语法（包含解析器和预处理器，解析器生成解析树，预处理器判断字段存在歧义）优化器：选择正确的索引进行优化执行执行器：执行具体sql返回结果 mysql的两个重要日志redo-log（重做日志）：固定大小的循环缓存，InnoDB使用，即使重启，只要记录到了redo-log就不会丢失。防止mysql意外。bin-log：归档日志，所有sql都会记录，并且采用追加，满了之后新开，有两种方式，一种是记录sql语句（statement），一种是row，记录出现的事件。如果只记录sql语句会导致主从同步上面存在问题，从库执行相同的sql得到效果不同，所以还有一种混合的方式，mysql会自动判断当前语句是否会造成主从不同步的情况，如果会，那么就使用row记录如果不会就是用sql记录，因为row记录会增加存储空间。 undo-log（回滚日志）：记录修改的状态和回滚信息，利用这个实现mvcc（多版本并发控制），系统会自动判断回滚日志什么时候会被删除。用于回滚操作。 两个日志记录的顺序：更新的行如果不在内存，从磁盘取出 -&gt; 修改内存中的值 -&gt; 写入redo-log状态为prepare -&gt; 写binlog -&gt; 提交事务redo-log进行commit 数据库的隔离级别读未提交：能读到别人未提交事务修改的数据读已提交：能读到别人提交事务之后修改的数据可重复读：在读已提交的基础上，当前事务读取第一次和第二次的结果相同串行化：读会加读锁，写会加写锁，读写冲突串行化执行 隔离级别通过视图实现，读未提交没有视图，读已提交每次sql执行创建一个视图，可重复读在开始之前创建一个视图，串行化直接加锁没有视图。 事务与隔离级别：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”；所以即使是可以重复读的隔离级别，更新数据时还是会进行当前读来保证别人已经提交的事务不被覆盖。 幻读：幻读是出现在范围查询，第二次查询之前，由于其他事务新增记录导致查询两次不同，区别于可重复读。InnoDB引入间隙锁来解决，锁住范围内的各个间隙。但是要注意间隙锁也容易导致死锁，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作，间隙锁之间都不存在冲突关系。 索引基础常见的索引类型有：哈希、数组、搜索树哈希用于等值查询，不适合范围查询；数组查询很快，但是更新效率低数据库使用N叉树降低树的层级，innodb使用的是B+树 在InnoDb中，主键索引又叫聚簇索引，非主键索引又叫二级索引主键索引可以拿到全部数据，而非主键索引只能拿到主键id通过回表查询来拿到数据如果一个数据页满了需要新增一个数据页也叫做页分裂性能下降并且空间利用率下降，所以使用自增主键更加合理 覆盖索引：当我们查询的时候只需要查询出id字段的时候就可以直接使用单个索引来完成，不需要进行回表操作，减少搜索次数。 最左前缀原则：当我们进行一个字段查询的时候，如果这个字段没有单独做索引，但是有别的联合索引包含这个字段，且刚好以这个字段开头，那么也可以进行匹配。所以在建立联合索引的时候需要考虑字段排序，这样就可以减少维护的索引个数。 索引下堆优化：mysql5.6之后，当查询的条件中包含索引中的字段，会优先对索引中的字段做判断，而非直接回表查询。 重建索引：当删除很多数据之后，由于索引没有被删除，所以会导致数据页有空洞，而且占用资源，这个时候可以考虑再低谷期重建索引alter table T engine=InnoDB。 唯一索引和普通索引：插入上面性能几乎没有区别，更新上面普通索引可以使用change buffer所以更加快一些，而唯一索引需要判断所以慢一些。选择还是需要根据业务出发去考虑。 合理设置前缀索引：索引可以设置只用前面几位，可以减少索引占用空间，同时设置时应保证合适的区分度。 锁相关全局锁：用于备份的时候，锁住整个库，防止备份过程中数据修改导致问题。表锁：有两种，一种是表锁，在引擎不支持行锁的时候使用，锁住之后不能进行增删改查；另一种是元数据锁，访问表的时候自动加上，读写锁。默认就是。 行锁：在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 间隙锁：专门用来解决幻读的问题，在可重复读的情况下才会生效。（间隙锁和行锁合称next-key lock）next-key lock锁的规则：加锁范围是前开后闭区间；查找过程中访问到的对象才会加锁；当遇到索引等值查询，如果是唯一索引，那么因为只可能有一行记录那么就退化为行锁；如果索引等值查询，发现没有满足情况，就只能退化为间隙锁去锁间隙；如果是范围查询那么就会查询到第一个不满足条件的情况为止。 死锁：当对于同一个表的多行数据进行修改的时候，容易出现死锁，相互等待。死锁可以通过死锁检测或者是超时回滚来解决，但是对于性能损失巨大，最好通过业务或者客户端优化处理。 count(*)：针对这个有特殊优化，但innodb没有直接记录行数，还是需要遍历计数，实在不行可以业务实现计数。 MyISAM不支持事务MyISAM不支持行锁在InnoDB中，每个数据页的大小默认是16KB。 order by的实现：在不用索引的时候，如果内存够用，那么会将查询全部查出来然后放到内存中快排，如果内部不够，使用磁盘进行排序后归并。更好的情况是去使用索引，因为存储的时候默认就是有顺序的，这样能减少排序从而加速。 无法使用索引的情况 如果对字段做了函数计算，就用不上索引了 如果触发隐式转换也用不上索引了 字符集不同触发转换也无法使用索引 查看相关命令show processlist命令查看Waiting for table metadata lock查看各个线程锁的情况 select from information_schema.innodb_trx\Gselect from t sys.innodb_lock_waits where locked_table=&#39;test&#39;.&#39;t&#39;\G可以查看具体是被那个线程锁住了 一些小的for update和lock in share modelock in share mode是意向共享锁，其他session可以读取相关记录，也可以继续加IS，但是无法修改for update是意向排他锁，其他session无法进行select…for update操作，也就是排除别的想要加排它锁的情况。两者都不会阻塞别的session进行的快照读。用法，lock in share mode用于两个表之间要保证一致性，a表的操作时要保证b表中的某条数据不能被修改；for update用于同一个表中的数据，a事务操作时不允许b事务进行修改。 在删除数据的时候尽量加limit。这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 不要一次性地用delete语句删除太多数据。其实，这就是一个典型的大事务场景。 sql慢的原因索引设计不合理sql设计不合理mysql索引自动选择错误 运维上的一些双主的时候，通过binlog上面的serverid记录来判断是否与自己相同，如果不同才会更新，避免循环复制 主备延迟的来源：备库机器性能差，备库查询压力大，大事务一直正在处理。mysql5.7采用并行复制的策略减少主备延迟 因为主备同步会存在延迟，所以在开发的时候一定要注意读取从库的时候不一定是最新的值。1、读取的时候读主库，最常用2、读取之前进行睡眠一段时间保证同步保证seconds_behind_master一定为0的时候才执行查询或者可以使用semi-sync replication，当从库收到binlog之后会返回主库一个ack，主库只有收到这个ack之后才认为事务完成 如何进行主备切换？？？？过程是怎么样的？ 对比位点Master_Log_File和Read_Master_Log_Pos，表示的是读到的主库的最新位点；Relay_Master_Log_File和Exec_Master_Log_Pos，表示的是备库执行的最新位点。如果位点相同可以认为已经同步 对比GTID集合确保主备无延迟： 如何判断一个数据库正常1、使用select进行查询，查询一个创建在mysql库中的表；容易实现，但是因为只是查询所以会漏掉一些错误条件，比如当磁盘满了，binlog写不进去了，但是可以读不能写。那么可以使用update来进行优化一下下。尝试去修改一个值来实现。 当出现误删除（delete）的时候，这个时候要指望binlog存放了数据，然后逆执行去恢复（Flashback），但是需要确保binlog_format=row 和 binlog_row_image=FULL。 预防才是关键：把sql_safe_updates参数设置为on。这样一来，如果我们忘记在delete或者update语句中写where条件，或者where条件里面没有包含索引字段的话，这条语句的执行就会报错。 如果是直接执行的drop的话，binlog也无能为力，因为log中没有存放删除的数据，这个时候只能依赖备份了，利用最近一次备份的数据进行恢复，然后进行binlog重放。 故意延迟复制的从库，弄一个故意延迟一个小时复制的从库，这样无论什么时候都能快速拿到一个小时前的数据。 账号权限很关键，没有权限去执行对应操作的sql就可以了 kill query +线程id，可以终止一个线程正在执行的sql语句 mysql采用的是边查边给的，查到就会发给客户端，而不是全部查到全部结果之后再发 使用join的时候一定要注意，使用是有条件的：当使用join的时候被驱动表能使用索引，那么是可以的，同时也需要注意，使用小表作为驱动表，这样能让扫描行数更加少一些，大表去走索引去。当使用join的时候如果不能走索引的情况，那么mysql会使用BNL算法，将驱动表的数据和被驱动表的数据加载到内存中，并且使用join_buffer来进行合并操作，但是这样扫描行会变的非常的巨大，所以这个时候如果表的数据太多就不适合使用。 mysql面试问题主从复制的原理与流程？ 主库将修改写入本地binlog中 从库将拉取主库binlog写入本地relay log中 从库读取relay log并执行（这里是单线程执行，不能并发，所以慢） innodb和myisam与区别innodb支持事务，myisam不支持innodb支持行锁，myisam支持表锁innodb支持mvcc，myisam不支持innodb支持外键，myisam不支持myisam不支持崩溃后安全恢复 innodb引擎的4大特性 插入缓冲insert buffer，change buffer；将一系列的操作缓存，然后一次性写到磁盘，目的还是为了减少随机IO带来性能损耗。 二次写：从innodb buffer pool中flush写文件之前存doublewrite buffer写到物理磁盘上共享表空间。 自适应哈希索引：当二级索引访问频繁的时候，会自动建立哈希索引来加速 预读 mysql索引方法有哪些B-Tree索引：利用二叉树的特性，同时优化磁盘io，然后查询更快，同时优化索引查询和排序Hash索引：基于hash实现，在hash冲突不高的情况下，速度快，但是对于范围查询和排序都不支持 mysql索引类型有哪些主键索引，普通索引（组合索引），唯一索引，全文索引，空间索引]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的垃圾回收]]></title>
    <url>%2F2019%2F07%2F04%2Fgolang%2Fbasic%2Fgc-easy-dis%2F</url>
    <content type="text"><![CDATA[最近垃圾分类的话题热度一下子就上去了，很多人因为垃圾分类的问题很头痛。因为垃圾这个话题，那我就想来说说Golang里面的垃圾，于是就有了这篇博客，golang中的垃圾回收。 现阶段网上针对golang垃圾回收的解析已经很多了，所以我也没有必要仔仔细细的一点点说，还是那个原则，用最直白的话告诉你，垃圾到底是怎么收的。 GC的意义首先本文后续都会使用 GC 代替垃圾回收这几个字。我们知道创建对象会给他分配内存资源，如果这个对象不使用了，而这个内存资源却一直被占用的话，那么我们的电脑很快就会被放满，所以需要将这些垃圾对象进行回收。 什么才是垃圾要回收，那么我们必须知道什么才是垃圾，什么不是垃圾。在我们看来，一个对象以后都不用了，就是垃圾。在程序看来，一个对象没有被引用了，就是垃圾。 GC的流程首先说明一下，下面说的停，都是STW，stop the world，全世界暂停，所有运行的都停下来了。这个流程可以由下面这张图展示： 第一个过程先告诉所有人，停一下，我来记录一下当前状态。 第二个过程告诉所有人，你们继续，该干嘛干嘛，我标记一下要用的对象一开始所有点是白色，首先从根节点出发，标记相连的点为灰色（相连证明有引用），并且将所有灰色的点存起来；然后遍历所有灰色的点，标记所有灰色的点相连的点为灰色，并且将自己标记为黑色；然后重复，直到没有点是灰色； 第三个过程告诉所有人，再停一下，在第二个过程中，因为所有人继续在工作，那么就会产生新的垃圾，因为第一个过程记录了状态，所以需要标记一下新的垃圾；然后清除所有白色的点，因为白色的点是没人引用的，也就是垃圾。 为什么要这样GC你一定会有这样的疑问： 为什么要暂停两次？ 为什么不直接标记？ 如果再标记的过程中不断的在创建新的对象，那么永远就标记不完了。 如果标记的过程中，原来的被标记的对象引用发生变更也会导致问题。 那么既然会导致那么多问题，为什么不直接停下来，标记完回收完了再开始呢？因为慢~ 所以这样GC的原因是既要保证GC正常执行，又要保证效率，不能停的时间太长。 第一个过程其实第一次停的时候，启动了一个写屏障 (write barrier)它需要记录后续过程中新创建的对象 第二个过程这个过程称为三色标记，有点类似广度优先搜索。 第三个过程这次是必须停，因为在第二个过程中引用会发生变化，从而需要停止后重新扫描一遍；然后关闭写屏障，最后再清理。 重 点 什么时候需要stw？ 开启写屏障时需要stw关闭写屏障前需要stw 什么时候是并发执行的？ 开启写屏障之后的标记过程与其他程序并发执行关闭写屏障之后的清扫过程与其他程序并发执行 GC的触发条件那毕竟GC还是需要STW的，虽然可能停止时间很短，但是对于程序来说，整个程序停止1秒那对于用户来说就是致命打击。所以GC肯定需要一个触发的条件，不能想来就来。 GC百分比这是一个触发的条件，默认GC百分比设置的是100，意思是，如果这次回收之后总共占用2M的内存，那么下次触发的条件时当超过4M的时候；同理，当这次回收之后总共占用4M，那么下次触发条件就是8M。 2分钟这个简单，当一定时间（2分钟）没有执行过GC就触发GC（称为GC forced）监控服务 sysmon 每隔 2 分钟就会检查一次垃圾 回收状态，如超出 2 分钟未曾触发，那就强制执行。 手动使用命令runtime.GC()手动触发GC 如何查询gc过程启动时加上命令GODEBUG=&quot;gctrace=1&quot;运行一段时间你会看到类似下面的输出12345gc 1 @351.002s 0%: 0.017+0.88+0.038 ms clock, 0.21+0/0.75/0.72+0.45 ms cpu, 4-&gt;4-&gt;2 MB, 5 MB goal, 12 Pgc 2 @351.022s 0%: 0.006+1.6+0.045 ms clock, 0.075+0/0.57/0.65+0.54 ms cpu, 8-&gt;8-&gt;7 MB, 9 MB goal, 12 Pgc 3 @351.030s 0%: 0.005+2.7+0.032 ms clock, 0.061+0/3.0/0.90+0.38 ms cpu, 15-&gt;15-&gt;13 MB, 16 MB goal, 12 Pgc 4 @351.042s 0%: 0.005+5.7+0.043 ms clock, 0.067+0/0.44/5.6+0.52 ms cpu, 29-&gt;29-&gt;25 MB, 30 MB goal, 12 Pgc 5 @351.069s 0%: 0.005+10+0.039 ms clock, 0.063+0/0.33/10+0.47 ms cpu, 57-&gt;57-&gt;49 MB, 58 MB goal, 12 P 1 表示第一次执行 @351.002s 表示程序执行的总时间 0% 垃圾回收时间占用的百分比 0.017+0.88+0.038 ms clock 垃圾回收的时间，分别stop-the-world (STW) sweep termination + concurrent mark and scan + and STW mark termination0.017表示mark阶段stw的时间，是第一次stw，短暂的那个。0.88表示并发标记和扫描的时间。0.038表示mark termination中stw的时间，第二次stw，长的那个。 0.10+0.23/5.4/12+0.40 ms cpu：按顺序分成三部分，0.10表示整个进程在mark阶段STW停顿时间(0.013 8)；0.23/5.4/12有三块信息，0.23是mutator assists占用的时间，5.4是dedicated mark workers+fractional mark worker占用的时间，12是idle mark workers占用的时间。这三块时间加起来会接近2.98(P的个数)；0.40 ms表示整个进程在markTermination阶段STW停顿时间(0.050 * 8)。 4-&gt;4-&gt;2 MB 按顺序分成三部分，4表示开始mark阶段前的heap_live大小；4表示开始markTermination阶段前的heap_live大小；2表示被标记对象的大小。 5 MB goal 表示下一次触发GC的内存占用阀值是5MB，等于2MB * 2，向上取整。 12 P 使用的Processor数量 总结以上就是在golang中垃圾回收的大致流程，总的来说使用三色标记法进行标记清除，并且标记时与程序运行并行，为了解决问题使用写屏障来记录标记过程中对象的变更。总来的来说也是为了提高垃圾回收的效率，并且尽可能的减少STW的时间。了解下来，与java的分代回收相比，golang中的回收算法理解起来更加简单一些。 参考文章https://studygolang.com/articles/21569https://spin.atomicobject.com/2014/09/03/visualizing-garbage-collection-algorithms/https://www.jianshu.com/p/8b0c0f7772dahttp://legendtkl.com/2017/04/28/golang-gc/]]></content>
      <categories>
        <category>golang基础</category>
      </categories>
      <tags>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不是我吹，你可能连defer都不清楚]]></title>
    <url>%2F2019%2F07%2F02%2Fgolang%2Fbasic%2Fdefer-dis%2F</url>
    <content type="text"><![CDATA[在golang中，对于defer，我之前的理解就是和java中的finally代码块一样，没什么难度，但是吧，当我最近看的一些神奇的问题，我就发现原来并非想的那么简单。 先举个栗子123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport "fmt"func main() &#123; fmt.Println(DeferFunc1(1)) fmt.Println(DeferFunc2(1)) fmt.Println(DeferFunc3(1)) DeferFunc4()&#125;func DeferFunc1(i int) (t int) &#123; t = i defer func() &#123; t += 3 &#125;() return t&#125;func DeferFunc2(i int) int &#123; t := i defer func() &#123; t += 3 &#125;() return t&#125;func DeferFunc3(i int) (t int) &#123; defer func() &#123; t += i &#125;() return 2&#125;func DeferFunc4() (t int) &#123; defer func(i int) &#123; fmt.Println(i) fmt.Println(t) &#125;(t) t = 1 return 2&#125; 请问这段代码输出的结果是什么？答案见文末如果你看完答对了，那么请直接点击右上角的关闭按钮，如果你答错了，你可以继续往下看了。下面会一步步介绍，到底为什么结果会是这样 基础知识函数的返回值初始化如 ： func DeferFunc1(i int) (t int) {其中返回值t int，这个t会在函数起始处被初始化为对应类型的零值并且作用域为整个函数。 defer的执行顺序虽然这边没有提及，但是还是要说一下，因为很多人学习defer的时候都会用到，就是当多个defer出现的时候，它是一个“栈”的关系，也就是先进后出。一个函数中，写在前面的defer会比写在后面的defer调用的晚。 defer与return谁先谁后return先，defer后这个可能会让人怀疑，后面会详细解释。 函数的返回与return在没有defer的情况下，其实函数的返回就是与return一致的，但是有了defer就不一样了。函数的返回其实是有两个步骤的，第一个当执行到return语句的时候123456func DeferFunc3(i int) (t int) &#123; defer func() &#123; t += i &#125;() return 2&#125; 这个时候会先将返回值t赋值为2，然后执行defer，完成之后才会真正返回外部调用者。 defer调用的三步走这个就是今天的重头戏了，defer这个语法其实一共有三个步骤。 将defer方法中的参数进行赋值。 将defer压入栈中。 当return或者是panic的时候依次出栈执行。后面会用实际的例子说明具体执行的情况。 解释有了上面的所有知识点，其实你就应该能明白上面输出的结果了。如果还不明白就看看下面的分析解释吧。 DeferFunc11234567func DeferFunc1(i int) (t int) &#123; t = i defer func() &#123; t += 3 &#125;() return t&#125; 首先上面是第一个方法 将返回值t赋值为传入的i，此时t为1 执行return语句将t赋值给t（等于啥也没做） 执行defer方法，将t + 3 = 4 函数返回 4因为t的作用域为整个函数所以修改有效。 DeferFunc21234567func DeferFunc2(i int) int &#123; t := i defer func() &#123; t += 3 &#125;() return t&#125; 第二个方法 创建变量t并赋值为1 执行return语句，注意这里是将t赋值给返回值，此时返回值为1（这个返回值并不是t） 执行defer方法，将t + 3 = 4 函数返回返回值1 可能这里就有点难理解了，修改一下代码你就明白了1234567func DeferFunc2(i int) (result int) &#123; t := i defer func() &#123; t += 3 &#125;() return t&#125; 上面的代码return的时候相当于将t赋值给了result，当defer修改了t的值之后，对result是不会造成影响的。 DeferFunc3123456func DeferFunc3(i int) (t int) &#123; defer func() &#123; t += i &#125;() return 2&#125; 首先执行return将返回值t赋值为2 执行defer方法将t + 1 最后返回 3 DeferFunc412345678func DeferFunc4() (t int) &#123; defer func(i int) &#123; fmt.Println(i) fmt.Println(t) &#125;(t) t = 1 return 2&#125; 这个分析的步骤要详细一些 初始化返回值t为零值 0 首先执行defer的第一步，赋值defer中的func入参t为0 执行defer的第二步，将defer压栈 将t赋值为1 执行return语句，将返回值t赋值为2 执行defer的第三步，出栈并执行因为在入栈时defer执行的func的入参已经赋值了，此时它作为的是一个形式参数，所以打印为0；相对应的因为最后已经将t的值修改为2，所以再打印一个2 源码一瞥那么 defer 在底层究竟是如何实现的呢？通过生成汇编代码我们可以看到下面这样的方法：CALL runtime.deferproc(SB)CALL runtime.deferreturn(SB)实际上来说当我们使用defer的使用就会调用runtime.deferproc，那么这个时候，就会将所有的参数赋值好，所有就像我们上面例子中看到的一样，在调用defer的时候参数会先计算好保存起来，然后挂载到G._defer中，最后deferreturn的时候进行执行相关的defer中的方法123456789101112131415161718func deferproc(siz int32, fn *funcval) &#123; // arguments of fn follow fn sp := getcallersp(unsafe.Pointer(&amp;siz)) argp := uintptr(unsafe.Pointer(&amp;fn)) + unsafe.Sizeof(fn) callerpc := getcallerpc(unsafe.Pointer(&amp;siz)) systemstack(func() &#123; d := newdefer(siz)&#125;)d.fn = fnd.pc = callerpcd.sp = spmemmove(add(unsafe.Pointer(d), unsafe.Sizeof(*d)), unsafe.Pointer(argp), uintptr(siz)) // deferproc returns 0 normally. // a deferred func that stops a panic makes the deferproc return 1. // the code the compiler generates always checks the return value and jumps to the // end of the function if deferproc returns != 0. return0()&#125; 总结看完，有的人肯定又要出来搞事了，说这个在实际中不会遇到的，实际中谁写这么蠢的代码。但是其实某些时候非常重要，当我们需要在defer中返回一些错误信息的时候，并且需要将这些信息给到调用者的时候，就需要注意变量的作用域以及执行顺序所带来的差异。 而且正因为这样的执行顺序，在实际中要记住：defer 最大的功能是 panic 后依然有效所以defer可以保证你的一些资源一定会被关闭，从而避免一些异常出现的问题。 参考例子来源于网络，自己做了修改和结合：https://stackoverflow.com/questions/52718143/is-golang-defer-statement-execute-before-or-after-return-statement 答案41302]]></content>
      <categories>
        <category>golang基础</category>
      </categories>
      <tags>
        <tag>defer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang之context]]></title>
    <url>%2F2019%2F06%2F27%2Fgolang%2Fsource-code%2Fcontext-code-view%2F</url>
    <content type="text"><![CDATA[当我们使用一些golang框架的时候，总能在框架中发现有个叫做context的东西。如果你之前了解过java的spring，那么你肯定也听说过其中有个牛逼的ApplicationContext。Context这个东西好像随时随地都在出现，在golang中也是非常重要的存在。今天我们就来看看这个神奇的Context。 定义 首先我们要知道什么是context？ 很多人把它翻译成上下文，其实这个是一个很难描述很定义的东西，对于这种东西，我习惯用功能去定义它。我的定义是：context是用于在多个goroutines之间传递信息的媒介。官方定义：At Google, we developed a context package that makes it easy to pass request-scoped values, cancelation signals, and deadlines across API boundaries to all the goroutines involved in handling a request. 用法同样的我们先来看看它的一些基本用法，大致了解它的使用。 传递信息123456func main() &#123; ctx := context.Background() ctx = context.WithValue(ctx, "xxx", "123") value := ctx.Value("xxx") fmt.Println(value)&#125; 其实传递消息很简单，只需要通过context.WithValue方法设置，key-value然后通过ctx.Value方法取值就可以了。 暂时不用关心context.Background()只要知道context有传递值的功能就可以了。 关闭goroutine在我们写golang的时候goroutine是一个非常常用的东西，我们经常会开一个goroutine去处理对应的任务，特别是一些循环一直处理的情况，这些goroutine需要知道自己什么时候要停止。我们常见的解决方案是使用一个channel去接收一个关闭的信号，收到信号之后关闭，或者说，需要一个标识符，每个goroutine去判断这个标识符的变更从而得知什么时候关闭。那么用context如何实现呢？ 12345678910111213141516171819202122232425262728293031323334func main() &#123; ctx, _ := context.WithTimeout(context.Background(), time.Second * 3) go func() &#123; go1(ctx) &#125;() go func() &#123; go2(ctx) &#125;() time.Sleep(time.Second * 5)&#125;func go1(ctx context.Context) &#123; for &#123; fmt.Println("1 正在工作") select &#123; case &lt;-ctx.Done(): fmt.Println("1 停止工作") return case &lt;-time.After(time.Second): &#125; &#125;&#125;func go2(ctx context.Context) &#123; for &#123; fmt.Println("2 正在工作") select &#123; case &lt;-ctx.Done(): fmt.Println("2 停止工作") return case &lt;-time.After(time.Second): &#125; &#125;&#125; 通过context.WithTimeout我们创建了一个3秒后自动取消的context；所有工作goroutine监听ctx.Done()的信号；收到信号就证明需要取消任务； 其实使用起来比较简单，让我们来看看内部的原理。 源码解析创建context.TODO()这个就是创建一个占位用的context，可能在写程序的过程中还不能确定后期这个context的作用，所以暂时用这个占位 context.Background()这个是最大的context，也就是根context，这里就有必要说一下context的整个构成了，context其实构成的是一棵树，Background为根节点，每次创建一个新的context就是创建了一个新的节点加入这棵树。 context.WithTimeout()比如这个方法，创建一个自动过期的context12345678910111213// WithTimeout returns WithDeadline(parent, time.Now().Add(timeout)).//// Canceling this context releases resources associated with it, so code should// call cancel as soon as the operations running in this Context complete://// func slowOperationWithTimeout(ctx context.Context) (Result, error) &#123;// ctx, cancel := context.WithTimeout(ctx, 100*time.Millisecond)// defer cancel() // releases resources if slowOperation completes before timeout elapses// return slowOperation(ctx)// &#125;func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) &#123; return WithDeadline(parent, time.Now().Add(timeout))&#125; 可以看到需要传入一个parent，和过期时间，新创建的context就是parent的子节点。123456789101112131415161718192021222324func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) &#123; if cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(d) &#123; // The current deadline is already sooner than the new one. return WithCancel(parent) &#125; c := &amp;timerCtx&#123; cancelCtx: newCancelCtx(parent), deadline: d, &#125; propagateCancel(parent, c) dur := time.Until(d) if dur &lt;= 0 &#123; c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() &#123; c.cancel(true, Canceled) &#125; &#125; c.mu.Lock() defer c.mu.Unlock() if c.err == nil &#123; c.timer = time.AfterFunc(dur, func() &#123; c.cancel(true, DeadlineExceeded) &#125;) &#125; return c, func() &#123; c.cancel(true, Canceled) &#125;&#125; 注意其中cancelCtx: newCancelCtx(parent),其实是创建了一个可以取消的ctx，然后利用time.AfterFunc来实现定时自动过期。 还有一个细节c.mu.Lock()defer c.mu.Unlock()这个mu来自：12345678type cancelCtx struct &#123; Context mu sync.Mutex // protects following fields done chan struct&#123;&#125; // created lazily, closed by first cancel call children map[canceler]struct&#123;&#125; // set to nil by the first cancel call err error // set to non-nil by the first cancel call&#125; 这个context因为有了锁，所以是并发安全的。 取消12345678910111213141516171819202122232425262728// cancel closes c.done, cancels each of c's children, and, if// removeFromParent is true, removes c from its parent's children.func (c *cancelCtx) cancel(removeFromParent bool, err error) &#123; if err == nil &#123; panic("context: internal error: missing cancel error") &#125; c.mu.Lock() if c.err != nil &#123; c.mu.Unlock() return // already canceled &#125; c.err = err if c.done == nil &#123; c.done = closedchan &#125; else &#123; close(c.done) &#125; for child := range c.children &#123; // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err) &#125; c.children = nil c.mu.Unlock() if removeFromParent &#123; removeChild(c.Context, c) &#125;&#125; 当达到过期时间或者调用cancelFunc的时候就会触发context的取消，然后看到上面的源码你就明白了，取消的时候有一个三个操作： c.mu.Lock() 加锁保证安全 close(c.done) 将done信道关闭，从而所有在观察done信道的goroutine都知道要关闭了 for child := range c.children 循环每个子节点，关闭每个子节点。我们知道context的结构是树状的，所以同时我们要注意父节点如果关闭会关闭子节点的context。 WithValue和Value1234type valueCtx struct &#123; Context key, val interface&#123;&#125;&#125; 首先valueCtx的结构如上所示，包含一个Context和key-val 123456789func WithValue(parent Context, key, val interface&#123;&#125;) Context &#123; if key == nil &#123; panic("nil key") &#125; if !reflect.TypeOf(key).Comparable() &#123; panic("key is not comparable") &#125; return &amp;valueCtx&#123;parent, key, val&#125;&#125; 其实这个方法很简单，就是创建了一个parent的拷贝，并且将对应的key和val放进去。 123456func (c *valueCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; if c.key == key &#123; return c.val &#125; return c.Context.Value(key)&#125; Value方法就更简单了，就是判断当前key是否匹配，如果不匹配就去子节点寻找。 案例最后我们来看看在实际的使用过程中，我们在哪里使用到了context，我举两个实际中常用的框架gin和etcd gingin是一个web框架，在web开发的时候非常实用。1234567891011121314func main() &#123; router := gin.Default() router.POST("/post", func(c *gin.Context) &#123; id := c.Query("id") page := c.DefaultQuery("page", "0") name := c.PostForm("name") message := c.PostForm("message") fmt.Printf("id: %s; page: %s; name: %s; message: %s", id, page, name, message) &#125;) router.Run(":8080")&#125; 其实很多web框架都有Context，他们都自己封装了一个Context，利用这个Context可以做到一个request-scope中的参数传递和返回，还有很多操作通通都可以用Context来完成。 etcd如果你没有了解过etcd你就可以把它想象成redis，它其实是一个分布式的k-v数据存储我们在使用etcd进行操作（put或del等）的时候，需要传入context参数12345678timeoutCtx, cancel := context.WithTimeout(context.Background(), 2 * time.Second)defer cancel()putResponse, err := client.Put(timeoutCtx, "aaa", "xxx")if err != nil &#123; fmt.Println(err) return&#125;fmt.Println(putResponse.Header.String()) 这里传入的context是一个超时自动取消的context，也就是说，当put操作超过两秒后还没有执行成功的话，context就会自动done，同时这个操作也将被取消。 因为我们在使用etcd的时候，如果当前网络出现异常，无法连接到节点，或者是节点数量不足的时候，都会出现操作被hang住，如果没有定时取消的机制，或者手动取消，那么当前goroutine会被一直占用。所以就利用context来完成这个操作。 总结 context在web开发中，你可以类比java中的ThreadLocal，利用它来完成一个request-scope中参数的传递 context可以用于多个goroutine之间的参数传递 context还可以作为完成信号的通知 context并发安全 其实，我们不仅要学到context的使用，还可以学到这样设计一个系统的优点，如果以后自己在设计一些框架和系统的时候可以有更多的想法。]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>context</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang之reflect]]></title>
    <url>%2F2019%2F06%2F25%2Fgolang%2Fsource-code%2Freflect-code-view%2F</url>
    <content type="text"><![CDATA[反射 —— 如果你之前学过一些别的语言，比如java可能就会了解，反射是一个传说中很厉害的操作，算是一个高级用法。而同时，很多人也会告诉你，反射是一个危险的操作，那么在golang中，反射又是怎么操作的呢？今天就来说说golang中的反射reflect。 反射的定义首先问问自己，你知道什么是反射吗？如果你有一个清楚的定义，证明你已经对反射非常熟悉了。官方的定义很官方，我就说说我的：反射，反射，从字面理解就是通过镜子（或类似的东西）看到自己。而在编程中，反射指的是在运行的过程中看到自己。在实际的编程过程中我们知道，创建的这个变量或者对象是什么类型或者是什么样子的，同时很容易能对它进行操作。而在运行过程中，程序没有我们的眼睛，它并不知道这个东西是怎么样的，这个时候就需要运用到反射。通过反射我可以知道自己长什么样子。 反射的使用reflect.TypeOf如果你对反射还是有些模糊，那么看下面这个最简单的例子1234func main() &#123; a := 1.3 fmt.Println("a的类型是", reflect.TypeOf(a))&#125; 输出1a的类型是 float64 是不是瞬间明白了，没错，反射没有那么复杂。你想想，作为程序自己，我运行中，我怎么知道a是什么类型，只能通过照镜子（反射）得到。 下面再说说一些更高级的用法。 reflect.ValueOf1234567func main() &#123; type MyInt int var x MyInt = 7 v := reflect.ValueOf(x) fmt.Println(v.Type()) fmt.Println(v.Kind())&#125; 输出12main.MyIntint 这里我们通过reflect.ValueOf方法拿到的v，其中v.Type()拿到的是它当前的类型，而v.Kind()可以拿到它最基本的类型。 Elem()1234567func main() &#123; a := 1.3 v := reflect.ValueOf(&amp;a) elem := v.Elem() elem.SetFloat(0.2) fmt.Println(a)&#125; 输出10.2 这里我们可以看到，通过反射拿到v使用v.Elem()方法可以拿到对应指针进行操作赋值 Field()1234567891011121314151617type MyData struct &#123; A int b float32&#125; func main() &#123; myData := MyData&#123; A: 1, b: 1.1, &#125; myDataV := reflect.ValueOf(&amp;myData).Elem() fmt.Println("字段a:", myDataV.Field(0)) fmt.Println("字段b:", myDataV.Field(1)) fmt.Println(myDataV) myDataV.Field(0).SetInt(2) fmt.Println(myDataV)&#125; 输出1234字段a: 1字段b: 1.1&#123;1 1.1&#125;&#123;2 1.1&#125; 这里我们可以看到，我们即使不知道一个结构体里面的情况，我们依旧可以通过Field方法获得其中的值，并且如果这个变量可以被外界访问那么还可以修改。 Interface()123456func main() &#123; a := 1.3 v := reflect.ValueOf(a) a1 := v.Interface().(float64) fmt.Println(a1)&#125; 1.31a的类型是 float64 反射之后的对象通过Interface还可以转换回来 反射的法则上面就是一些反射的基本用法，常用的就是获取一个对象在运行中的一个状态，或者是针对运行中的一个不确定的对象进行修改。下面要说的是反射的法则。如果你英文够好，并且网络自由，可以看看golang官方的博客：https://blog.golang.org/laws-of-reflection里面详细描述了反射的三个法则，如果你看不到，那就只能听我下面吹一吹了。 Reflection goes from interface value to reflection object. Reflection goes from reflection object to interface value. To modify a reflection object, the value must be settable. 这三个就是官方给出的法则，我分别用自己的话解释一下。 反射就是将任意值转换为反射对象在golang中我们知道interface就和java中的Object类似（只是类似而已），代表了所有类型，reflect包正是帮我们将任意的一个类型转换成了我们上面例子中看到的一个v，这个v就是反射对象。通过这个反射对象中的一些方法我们才能看见原来的对象是什么样子。 反射对象可以转换为任意对象这个正好与第一个相反，就像最后一个例子中给出的，反射获得的反射对象可以通过Interface方法转换为原来的对象。 如果你要修改反射对象，那么这个对象必须要可以被修改什么意思呢？就如同这个案例中1234567func main() &#123; a := 1.3 v := reflect.ValueOf(&amp;a) elem := v.Elem() elem.SetFloat(0.2) fmt.Println(a)&#125; 如果我们传递的并非a的地址并且直接使用v.SetFloat那么就会报错，因为我们无法对其进行修改，反射会帮我们copy一个，所以无法修改，只有当我们使用指针的时候才能修改。 同样的，和案例中的结构体操作一样，如果一个结构体的变量是小写的而不是大写的，证明外界没有办法访问到，所以也没有办法修改，也会出现异常。 反射的原理下面就需要看看在源码中，反射到底是怎么实现的了。我们着重看两个方法TypeOf和ValueOf TypeOf123456// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface&#123;&#125;) Type &#123; eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ)&#125; 我们先来看这个简单的TypeOf看到源码中很简单，通过unsafe.Pointer获得指针转换成emptyInterface类型12345// emptyInterface is the header for an interface&#123;&#125; value.type emptyInterface struct &#123; typ *rtype word unsafe.Pointer&#125; 然后通过toType方法得到具体类型123456func toType(t *rtype) Type &#123; if t == nil &#123; return nil &#125; return t&#125; 其中Type就包含了所有的信息，然后返回出去就完成了。 ValueOf123456789101112131415// ValueOf returns a new Value initialized to the concrete value// stored in the interface i. ValueOf(nil) returns the zero Value.func ValueOf(i interface&#123;&#125;) Value &#123; if i == nil &#123; return Value&#123;&#125; &#125; // TODO: Maybe allow contents of a Value to live on the stack. // For now we make the contents always escape to the heap. It // makes life easier in a few places (see chanrecv/mapassign // comment below). escapes(i) return unpackEface(i)&#125; 上面nil就不说了，主要方法是下面unpackEface1234567891011121314// unpackEface converts the empty interface i to a Value.func unpackEface(i interface&#123;&#125;) Value &#123; e := (*emptyInterface)(unsafe.Pointer(&amp;i)) // NOTE: don't read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil &#123; return Value&#123;&#125; &#125; f := flag(t.Kind()) if ifaceIndir(t) &#123; f |= flagIndir &#125; return Value&#123;t, e.word, f&#125;&#125; 我们可以看到其实与TypeOf一样，只不过多封装了一层Value，其中的word就是当前对象的指针，因为我们知道通过TypeOf得到的Value可以用很多操作。12345// emptyInterface is the header for an interface&#123;&#125; value.type emptyInterface struct &#123; typ *rtype word unsafe.Pointer&#125; 反射的意义说了这么多，那么反射存在的意义到底在哪？说白了，我们在写代码的时候什么时候能用上它？还是举个例子你就明白了。 json.Marshal案例json.Marshal这个方法用过吧，是将任意对象转换成json，这个案例就足以说明反射的厉害了。我们先自己想一下，如果要将一个对象转换成json： 我们运行之前其实是不知道传入对象的类型，而且传入的对象不同，那么解析方式肯定不同，如果传入的是map或者传入的是struct肯定解析方式不同，所以方法内部需要动态的判断传入类型从而做操作。 还有我们不知道传入的struct内部的属性长什么样子。 这个时候反射就能解决这样的问题，通过反射可以知道传入对象的类型，根据不同的类型做操作，同时可以获取到如struct这样类型内部的字段属性和值分别是多少。 json.Marshal源码分析因为所有源码太多，我给出查看路线，然后给出上面所述的两处重点。json.Marshal -&gt; e.marshal -&gt; e.reflectValue -&gt; valueEncoder -&gt; typeEncoder -&gt; newTypeEncoder -&gt; newStructEncoder -&gt; se.encode 要点1 - newTypeEncoder12345678910111213141516171819202122232425262728293031323334// newTypeEncoder constructs an encoderFunc for a type.// The returned encoder only checks CanAddr when allowAddr is true.func newTypeEncoder(t reflect.Type, allowAddr bool) encoderFunc &#123; ........... switch t.Kind() &#123; case reflect.Bool: return boolEncoder case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return intEncoder case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr: return uintEncoder case reflect.Float32: return float32Encoder case reflect.Float64: return float64Encoder case reflect.String: return stringEncoder case reflect.Interface: return interfaceEncoder case reflect.Struct: return newStructEncoder(t) case reflect.Map: return newMapEncoder(t) case reflect.Slice: return newSliceEncoder(t) case reflect.Array: return newArrayEncoder(t) case reflect.Ptr: return newPtrEncoder(t) default: return unsupportedTypeEncoder &#125;&#125; 通过反射获得传入对象的类型，判断选择具体的编码器进行编码，如果传入的是map那就…如果传入的是struct那就… 要点2 - se.encode1234567891011121314151617181920func (se *structEncoder) encode(e *encodeState, v reflect.Value, opts encOpts) &#123; e.WriteByte('&#123;') first := true for i, f := range se.fields &#123; fv := fieldByIndex(v, f.index) if !fv.IsValid() || f.omitEmpty &amp;&amp; isEmptyValue(fv) &#123; continue &#125; if first &#123; first = false &#125; else &#123; e.WriteByte(',') &#125; e.string(f.name, opts.escapeHTML) e.WriteByte(':') opts.quoted = f.quoted se.fieldEncs[i](e, fv, opts) &#125; e.WriteByte('&#125;')&#125; 123456789101112func fieldByIndex(v reflect.Value, index []int) reflect.Value &#123; for _, i := range index &#123; if v.Kind() == reflect.Ptr &#123; if v.IsNil() &#123; return reflect.Value&#123;&#125; &#125; v = v.Elem() &#125; v = v.Field(i) &#125; return v&#125; encode这个方法是解析结构体的，我们可以清楚的看的从结构体中通过v.Field将里面的参数拿出来。其他细节这里就不做说明了，主要的目的是要表示反射在其中起到的重要作用。 总结和提醒看完你就应该清楚反射到底是做什么用的，具体我们什么时候会用到它。最后还要提醒一下，反射也存在两个必然的问题： 第一个是不安全，因为反射的类型在转换中极易出现问题，所以使用需谨慎。 第二个是速度慢，之所以有人抨击golang的json解析库慢，一部分原因就是因为其中涉及到了反射，所以如果对效率有要求的地方就要斟酌使用了。]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>reflect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的strings.go源码解析 - Rabin-Karp了解一下？]]></title>
    <url>%2F2019%2F06%2F20%2Fgolang%2Fsource-code%2Fstrings-go-source-code%2F</url>
    <content type="text"><![CDATA[strings包是我们经常在处理字符串的时候要用的，这次我们来看看它其中的一些方法具体是如何实现的。我就找到其中常用的几个方法，然后针对其中比较难的部分还有应用到一些特别算法的部分进行分析。 ToUpper先来看个简单的ToUpper，将所有字符转换成大写。这个如果让我们自己实现也没有什么难度，就是遍历每个字符转换成大写就可以。 12345678910111213141516171819202122232425262728// ToUpper returns a copy of the string s with all Unicode letters mapped to their upper case.func ToUpper(s string) string &#123; isASCII, hasLower := true, false for i := 0; i &lt; len(s); i++ &#123; c := s[i] if c &gt;= utf8.RuneSelf &#123; isASCII = false break &#125; hasLower = hasLower || (c &gt;= 'a' &amp;&amp; c &lt;= 'z') &#125; if isASCII &#123; // optimize for ASCII-only strings. if !hasLower &#123; return s &#125; b := make([]byte, len(s)) for i := 0; i &lt; len(s); i++ &#123; c := s[i] if c &gt;= 'a' &amp;&amp; c &lt;= 'z' &#123; c -= 'a' - 'A' &#125; b[i] = c &#125; return string(b) &#125; return Map(unicode.ToUpper, s)&#125; 可以看到，源码中考虑的比较周全，判断了一下字符集的问题。 小技巧c -= ‘a’ - ‘A’我们可以看到，这个转换大写的技巧，学习一下。如果转换成小写，只要改成+就可以了。 Replace然后看一个稍微复杂一些的，Replace，这个函数的目标是替换s中old的字符，替换前n个，如果n为负数则全部替换。作为实现也其实不难，就是找到对应字符替换就可以。 123456789101112131415161718192021222324252627282930313233343536373839// Replace returns a copy of the string s with the first n// non-overlapping instances of old replaced by new.// If old is empty, it matches at the beginning of the string// and after each UTF-8 sequence, yielding up to k+1 replacements// for a k-rune string.// If n &lt; 0, there is no limit on the number of replacements.func Replace(s, old, new string, n int) string &#123; if old == new || n == 0 &#123; return s // avoid allocation &#125; // Compute number of replacements. if m := Count(s, old); m == 0 &#123; return s // avoid allocation &#125; else if n &lt; 0 || m &lt; n &#123; n = m &#125; // Apply replacements to buffer. t := make([]byte, len(s)+n*(len(new)-len(old))) w := 0 start := 0 for i := 0; i &lt; n; i++ &#123; j := start if len(old) == 0 &#123; if i &gt; 0 &#123; _, wid := utf8.DecodeRuneInString(s[start:]) j += wid &#125; &#125; else &#123; j += Index(s[start:], old) &#125; w += copy(t[w:], s[start:j]) w += copy(t[w:], new) start = j + len(old) &#125; w += copy(t[w:], s[start:]) return string(t[0:w])&#125; 其实核心就是下面三句w += copy(t[w:], s[start:j])w += copy(t[w:], new)start = j + len(old)利用一个start去标记从旧字符串的那个位置开始复制，到目标字符处，然后复制需要替换的字符进去就可以了，最后移动start为了下一次准备就可以了。 小技巧t := make([]byte, len(s)+n*(len(new)-len(old)))这个在源码中很是常见，告诉我们一个道理，在创建slice的时候，尽可能的去指定好你需要的长度来避免扩容。 Index好了，热身差不多了，来看我们这次的重头戏，index。我们经常需要确定一个字符串是否存在于另一个字符串内，并且要知道它的位置，所以需要index方法。其实说到底就是字符串匹配嘛。 自己思考一般看源码我都习惯先自己想想怎么去实现，这个方法对于我来说很熟悉，在java中其实很暴力，就是两层for搞定，先找到第一个一样的字符，然后匹配剩下的。然后我也知道，字符串匹配在算法中有著名的KMP算法，但是理解难度很大。不知道golang会怎么实现，于是我看到了一个新的算法RabinKarp（我之前不了解） 源码1234567891011121314151617181920212223func indexRabinKarp(s, substr string) int &#123; // Rabin-Karp search hashss, pow := hashStr(substr) n := len(substr) var h uint32 for i := 0; i &lt; n; i++ &#123; h = h*primeRK + uint32(s[i]) &#125; if h == hashss &amp;&amp; s[:n] == substr &#123; return 0 &#125; for i := n; i &lt; len(s); &#123; h *= primeRK h += uint32(s[i]) h -= pow * uint32(s[i-n]) i++ if h == hashss &amp;&amp; s[i-n:i] == substr &#123; return i - n &#125; &#125; return -1&#125; 你先自己尝试看看是否能看出什么门道？ 猜测是不是乍看之下这个方法很复杂，各种操作眼花缭乱，如果你是第一次看源码可能是这样的，看多了你应该有和我一样的直觉和经验（反正我是有感觉）我看源码的第二个步骤就是大致看一眼，然后在不看任何文章解析的情况下猜测它的实现。在我看完上面之后留个我三个重点 hashss, pow := hashStr(substr) h += uint32(s[i]) h -= pow * uint32(s[i-n]) if h == hashss &amp;&amp; s[i-n:i] == substr { 它获取了对应的hash值，这个算法和hash有关 它对哈希值进行了增减操作 它比较哈希值和字符串从而确定位置 到这里，我已经有了一个大概的思路，这个算法应该是通过哈希值快速确定子串是否可能存在，在哈希值相同的情况下再去比较真实的字符是否一致，同时在计算哈希值的时候采用特殊的机制来实现了增加就可以完成哈希的改变。其实如果你有相同的想法，恭喜你，已经八九不离十了。 分析我们举个实际的例子来说明上面的事情原本的字符串为：”ABCDE”，子串为”BCD”首先计算出”BCD”的hash为100（举个例子）然后循环原本的字符串取出”ABC”计算hash为200 != 100所以一定不是取出字符D原来的hash加D减去A计算hash为100 == 100（这里注意，不是重新计算，而是在原有的基础上进行的计算）最后还是要比较一遍是否正确，因为hash一致不一定原值一致。 假设待匹配字符串的长度为M，目标字符串的长度为N，那么一共比较N-M+1就可以了 hash那么其实你应该注意到了，最神奇的就是这个hash值，为什么可以这样操作，如果是md5这种必须重新计算，是不可能完成这样的操作的。 12345678910111213141516171819// primeRK is the prime base used in Rabin-Karp algorithm.const primeRK = 16777619// hashStr returns the hash and the appropriate multiplicative// factor for use in Rabin-Karp algorithm.func hashStr(sep string) (uint32, uint32) &#123; hash := uint32(0) for i := 0; i &lt; len(sep); i++ &#123; hash = hash*primeRK + uint32(sep[i]) &#125; var pow, sq uint32 = 1, primeRK for i := len(sep); i &gt; 0; i &gt;&gt;= 1 &#123; if i&amp;1 != 0 &#123; pow *= sq &#125; sq *= sq &#125; return hash, pow&#125; 这个就是hash算法，其实它每次完成的就是 *primeRK 加上新的字符，那么pow是什么呢？加减究竟是如何完成的呢？我举个例子你就明白了。 还是刚才的ABCDE和BCD，我们用q表示常数primeRK那么BCD的hash计算出来应该是B q^2 + C q + D而ABC的hash应该是A q^2 + B q + C那当D来的时候如何操作的呢？回看一下indexRabinKarp就明白了。[A q^2 + B q + C] q + D - A pow= A q^3 - A pow + B q^2 + C q + D明白了吧，这个pow其实就是计算hash值时的最高指数，通过每次减去常数的最高指数项就能完成之前的操作。 聪明。不由得佩服能想出这样算法的人~ 其他一些方法看完最复杂的index实现，再说说几个由它引申出来的方法。 genSplit其实就是分组字符串，通过某些子串去分割成一个个部分，其实实现就是每次Index找到位置然后进行存储到新的地方就可以了。123456789for i &lt; n &#123; m := Index(s, sep) if m &lt; 0 &#123; break &#125; a[i] = s[:m+sepSave] s = s[m+len(sep):] i++&#125; countGeneric计数，统计字符串中子串出现的数目，也是通过index完成，统计一下而已12345678for &#123; i := Index(s, substr) if i == -1 &#123; return n &#125; n++ s = s[i+len(substr):]&#125; 总结其实很多源码中的实现不复杂，多看看，不仅能熟练使用api还能学到一些骚操作，何乐而不为呢？不得不佩服一些牛逼的算法实现，真的厉害~]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>strings</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的slice]]></title>
    <url>%2F2019%2F06%2F18%2Fgolang%2Fsource-code%2Fslice-source-code-review%2F</url>
    <content type="text"><![CDATA[今天来说个简单的，也不简单的东西，那就是切片。slice对于golang来说那真的是一个非常常用的东西了，很多地方都会用到它，今天就来说说，slice底层是如何实现的，又有哪些坑是需要提前注意的。 slice结构很多第一次接触golang的同学都会认为，数组和切片是差不多的东西，其实不是的，切片是数组的封装。12345type slice struct &#123; array unsafe.Pointer len int cap int&#125; 上面这个就是slice的结构，顺便说一下：slice的源码位置是：go/src/runtime/slice.go 其中array是一个指针，指向底层的数组 len代表slice的长度 cap代表slice的容量 为什么会有长度和容量这个区分呢，这两个东西是用来干什么的呢？我们往下看。 slice的长度和容量我们先来看一个最简单的案例1234sli := make([]int, 2)fmt.Printf("len=%d cap=%d\n", len(sli), cap(sli))sli = append(sli, 1)fmt.Printf("len=%d cap=%d\n", len(sli), cap(sli)) 我们创建一个长度为2的slice然后打印一下它的len和cap。然后添加一个元素，再次打印最后结果为：len=2 cap=2len=3 cap=4 从中我们可以知道len和cap是不同的东西，明显嘛。但是为什么呢？ 其实原因很简单，因为数组在创建的时候只能创建固定大小的数组，而当slice在不断往其中添加元素的时候，势必会遇到大小不够的情况，如果每次添加都不够，那么每次都要创建新的数组，那会相当浪费时间和资源，所以当不够的时候索性一次就创建大一些，所以cap其实就代表了整体的一个容量，而len代表当前用到了第几个。 slice的扩容刚才提到的整个过程就是扩容的原因，那么slice究竟是如何进行扩容的呢？网上我看见过两个说法： 每次2倍 当len&lt;1024的时候每次2倍，当len&gt;1024的时候每次1.25倍 我最后得到的结论是其实两个都不完全正确。正确的应该看看源码中是怎么说的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func growslice(et *_type, old slice, cap int) slice &#123; ..... newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; newcap = cap &#125; else &#123; if old.len &lt; 1024 &#123; newcap = doublecap &#125; else &#123; // Check 0 &lt; newcap to detect overflow // and prevent an infinite loop. for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap &lt;= 0 &#123; newcap = cap &#125; &#125; &#125; var overflow bool var lenmem, newlenmem, capmem uintptr const ptrSize = unsafe.Sizeof((*byte)(nil)) switch et.size &#123; case 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; _MaxMem newcap = int(capmem) case ptrSize: lenmem = uintptr(old.len) * ptrSize newlenmem = uintptr(cap) * ptrSize capmem = roundupsize(uintptr(newcap) * ptrSize) overflow = uintptr(newcap) &gt; _MaxMem/ptrSize newcap = int(capmem / ptrSize) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem = roundupsize(uintptr(newcap) * et.size) overflow = uintptr(newcap) &gt; maxSliceCap(et.size) newcap = int(capmem / et.size) &#125; ...... return slice&#123;p, old.len, newcap&#125;&#125; 我们省略其中部分代码看关键部分，首先说明一下growslice这个方法是扩容的方法，其中的入参et *_type, old slice, cap int分别是元素的类型，老的slice，新slice要求的最小容量针对最后这个参数举个简单的例子，当前如果是len=2，cap=2的一个slice添加一个元素，那么这个参数传入的就是3，因为最小需要容量为3。 其实从前半部分来看，第二种说法是正确的，当len&lt;1024确实就是两倍，而当len&gt;1024的时候，每次以原来的25%增加直到满足要求。 但是其实你看后面部分，有一个roundupsize的方法，并且又对newcap进行赋值，所以肯定修改了cap的值，所以其实扩容并没有描述的那么简单，实际中会进行内存对齐，具体什么事内存对齐呢？简单的描述是，内存中肯定不是你想怎么放就怎么放的肯定要满足一个规则，有的地方虽然你只要这么点地方，但是由于美观的要求，会多给你一点，凑个整，保持统一整齐，这就是内存对齐。（是不是花里胡哨的，我尽可能已经白话了，具体还是要看）https://blog.csdn.net/u011957758/article/details/85059117总之，我们知道，slice的扩容并不是那么简单的。最后附上一个例子作为验证：123456789101112func main() &#123; sli := make([]int, 2) preCap := cap(sli) for i := 0; i &lt;= 2048; i++ &#123; sli = append(sli, i) if cap(sli) != preCap &#123; fmt.Printf("len=%4d \t cap=%d\n", len(sli)-1, cap(sli)) preCap = cap(sli) &#125; &#125;&#125; 输出123456789101112len= 2 cap=4len= 4 cap=8len= 8 cap=16len= 16 cap=32len= 32 cap=64len= 64 cap=128len= 128 cap=256len= 256 cap=512len= 512 cap=1024len=1024 cap=1280len=1280 cap=1696len=1696 cap=2304 1280 = 1024 1.251696 = 1280 1.325 slice的操作普通的创建添加元素我就不多说了，你肯定知道，你要是不知道就不会来看我的博客了。说一些看起来高端的微操。 创建slicemake([]int, 10, 32)make的时候可以指定第三个参数也就是初始的cap slice删除一个元素sli = append(sli[:3], sli[4:]…)因为底层是数组，所以删除一个元素看起来会比较麻烦 reslice123456789101112func main() &#123; sli := make([]int, 0) for i := 1; i &lt;= 10; i++ &#123; sli = append(sli, i) &#125; fmt.Println(sli) sli = sli[1:3:5] fmt.Println(sli) fmt.Println(len(sli), cap(sli))&#125; sli = sli[1:3:5]reslice的时候也可以指定cap，但是注意的是，这个时候并不是指定的整体容量为5，而是容量为原来slice下标为5的地方。如果原来是[1 2 3 4 5 6 7 8 9 10]按照上面的操作，按照我自己的平常说的就是切两刀，第一刀是切到3，第二刀是切到5slice是[2,3]，但是实际底层还有[4,5] cap应该是4，所以输出应该是： 123[1 2 3 4 5 6 7 8 9 10][2 3]2 4 slice的坑点slice的坑其实主要在于使用者需要清楚值传递引用传递的关系。首先在golang中只有值传递，没有引用传递。 reslice的时候要注意，如果只是reslice那么后续操作是会对原来的slice造成影响的。 但是如果经过append之后，那么由于扩容的时候回重新分配内存，如果涉及扩容之后，那么就不会对原来的slice造成影响。 如果作为函数的参数传递的是数组，因为是值传递，所以函数内部的修改不会对外部的变量产生影响，但是如果是slice传递，那么因为传递的是指针，所以会修改外部的变量。 同时因为是值传递，形参的重新赋值是不会对外部的变量造成影响的。 下面的代码说明了以上可能出现的坑点1234567891011121314151617181920212223242526272829303132333435363738package mainimport "fmt"func main() &#123; a := [3]int&#123;1, 1, 1&#125; s := []int&#123;1, 1, 1&#125; modifyArray(a) fmt.Println(a) modifySlice(s) fmt.Println(s) reslice(s) fmt.Println(s) s1 := make([]int, 2, 3) s2 := append(s1, 1) s2[0] = 3 fmt.Println(s1) s3 := append(s1, 1, 1) s3[0] = 4 fmt.Println(s1)&#125;func modifyArray(a [3]int) &#123; a[0] = 2&#125;func modifySlice(s []int) &#123; s[0] = 2&#125;func reslice(s []int) &#123; s = s[:2]&#125; 总结总结一下，创建slice的时候如果可以的话尽可能初始化好要用容量，以免经常扩容。slice作为参数进行传递的时候，还有slice进行append的时候注意一下，别的应该没有问题。总的来说slice的实现还是比较简单的。]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入深出ETCD之【集群部署与golang客户端使用】]]></title>
    <url>%2F2019%2F06%2F14%2Fgolang%2Fopen-source-component%2Fetcd-cluster-client%2F</url>
    <content type="text"><![CDATA[之前说了etcd的简介，命令行使用，一些基本原理。这次来说说现实一点的集群部署和golang版本的客户端使用。因为在实际使用过程中，etcd的节点肯定是需要2N+1个进行部署的，所以有必要说明一下集群的部署。 集群部署网上有很多集群部署的教程，有的很复杂，其实对于我们实际使用来说，其实配置并不复杂，下面举例一种最简单的集群配置。（简单到你想不到~） 下载https://github.com/etcd-io/etcd/releases还是在github上面找到需要下载的版本我使用的是etcd-v3.3.13-linux-amd64.tar.gz使用wget下载到linux你喜欢的目录，或者本地下载完成之后上传均可。 部署首先我找了三台机器，对应ip为192.168.4.224192.168.4.225192.168.4.226PS：提醒一下记得开发对应防火墙的端口 然后将下载的文件解压，之后进入解压后的目录，分别使用下面的命令启动。(注意下面的命令对应的是三台不同的机器，你需要修改对应为你自己的ip) 1234567891011121314151617181920212223$ ./etcd --name infra0 --initial-advertise-peer-urls http://192.168.4.224:2380 \--listen-peer-urls http://192.168.4.224:2380 \--listen-client-urls http://192.168.4.224:2379,http://127.0.0.1:2379 \--advertise-client-urls http://192.168.4.224:2379 \--initial-cluster-token etcd-cluster-1 \--initial-cluster infra0=http://192.168.4.224:2380,infra1=http://192.168.4.225:2380,infra2=http://192.168.4.226:2380 \--initial-cluster-state new$ ./etcd --name infra1 --initial-advertise-peer-urls http://192.168.4.225:2380 \--listen-peer-urls http://192.168.4.225:2380 \--listen-client-urls http://192.168.4.225:2379,http://127.0.0.1:2379 \--advertise-client-urls http://192.168.4.225:2379 \--initial-cluster-token etcd-cluster-1 \--initial-cluster infra0=http://192.168.4.224:2380,infra1=http://192.168.4.225:2380,infra2=http://192.168.4.226:2380 \--initial-cluster-state new$ ./etcd --name infra2 --initial-advertise-peer-urls http://192.168.4.226:2380 \--listen-peer-urls http://192.168.4.226:2380 \--listen-client-urls http://192.168.4.226:2379,http://127.0.0.1:2379 \--advertise-client-urls http://192.168.4.226:2379 \--initial-cluster-token etcd-cluster-1 \--initial-cluster infra0=http://192.168.4.224:2380,infra1=http://192.168.4.225:2380,infra2=http://192.168.4.226:2380 \--initial-cluster-state new 至此，三个节点的集群部署完成。😂😂😂没错就是这么easy，没有网上说的那么复杂。 配置文件如果你嫌弃每次使用这么长的命令进行启动，你可以将它写为配置文件：12345678910111213141516171819202122# 当前节点名称name: infra1# etcd数据保存目录data-dir: /usr/local/etcd# 供外部客户端使用的urllisten-client-urls: http://192.168.4.225:2379,http://127.0.0.1:2379# 广播给外部客户端使用的urladvertise-client-urls: http://192.168.4.225:2379# 集群内部通信使用的URLlisten-peer-urls: http://192.168.4.225:2380# 广播给集群内其他成员访问的URLinitial-advertise-peer-urls: http://192.168.4.225:2380# 集群的名称initial-cluster-token: etcd-cluster-1# 初始集群成员列表initial-cluster: infra0=http://192.168.4.224:2380,infra1=http://192.168.4.225:2380,infra2=http://192.168.4.226:2380#初始集群状态initial-cluster-state: new 然后指定配置文件的路径进行启动就可以了1./etcd --config-file=conf.yml 其他部署策略以上的部署一方面，我个人部署时使用的最简单方式，更简单的可能是使用yum进行etcd的下载。当然上述方式也存在一些问题，现在的etcd相当于裸奔的情况： 没有鉴权就想到于任何人知道ip和端口就可以连接上你的etcd，所以当前可能只适用于内网使用，服务通过内网ip进行访问（这个可以通过添加权限和用户来完成） 当前通信是没有加密的 当前etcd是利用静态ip来进行配置的，我认为这也是实际中用到最普通的情况，但是etcd还提供发现机制来进行部署和配置，更加灵活 等等，这些部署策略更多针对于线上，因为官方写的非常详细了，我感觉再写也就班门弄斧了。https://doczhcn.gitbook.io/etcd/index/index-1/clustering Golang客户端使用这里来实际用代码操作一下etcd，还是和之前使用命令行一样，get/put/del/watch/lease用一下这些操作，其他操作请查看dochttps://godoc.org/github.com/coreos/etcd/clientv3 客户端下载这里不建议使用go get进行下载，真的太慢了，可以直接从github上面下载之后放到对应目录快一些。https://github.com/etcd-io/etcd下载解压之后放到gopath下对应：go/src/go.etcd.io/etcd 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package mainimport ( "context" "fmt" "go.etcd.io/etcd/clientv3" "go.etcd.io/etcd/mvcc/mvccpb" "time")func main() &#123; // 配置客户端连接 client, err := clientv3.New(clientv3.Config&#123; // Endpoints: []string&#123;"127.0.0.1:2379"&#125;, Endpoints: []string&#123;"192.168.4.224:2379", "192.168.4.225:2379", "192.168.4.226:2379"&#125;, DialTimeout: 5 * time.Second, &#125;) if err != nil &#123; panic(err) &#125; defer client.Close() // 启动watch监听 watch := client.Watch(context.TODO(), "aaa") go func() &#123; for &#123; watchResponse := &lt;- watch for _, ev := range watchResponse.Events &#123; switch ev.Type &#123; case mvccpb.DELETE: fmt.Printf("监听到del：%s\n", ev.Kv.Key) case mvccpb.PUT: fmt.Printf("监听到put：%s, %s\n", ev.Kv.Key, ev.Kv.Value) &#125; &#125; &#125; &#125;() // 新增 putResponse, err := client.Put(context.TODO(), "aaa", "xxx") if err != nil &#123; fmt.Println(err) return &#125; fmt.Println(putResponse.Header.String()) // 查询 getResponse, err := client.Get(context.TODO(), "aaa") if err != nil &#123; fmt.Println(err) return &#125; fmt.Println(getResponse.Kvs) // 删除 deleteResponse, err := client.Delete(context.TODO(), "aaa") if err != nil &#123; fmt.Println(err) return &#125; fmt.Println(deleteResponse.Header.String()) // 申请租约 grantResponse, err := client.Grant(context.TODO(), 10) if err != nil &#123; fmt.Println(err) return &#125; // 使用租约 response, err := client.Put(context.TODO(), "aaa", "xxx", clientv3.WithLease(grantResponse.ID)) if err != nil &#123; fmt.Println(err) return &#125; fmt.Println(response.Header.String()) // 等待租约自动过期 time.Sleep(time.Second * 20)&#125; 大致能得到以下输出 监听到put：aaa, xxxcluster_id:14841639068965178418 member_id:10276657743932975437 revision:53 raft_term:4[key:”aaa” create_revision:53 mod_revision:53 version:1 value:”xxx” ]监听到del：aaacluster_id:14841639068965178418 member_id:10276657743932975437 revision:54 raft_term:4监听到put：aaa, xxxcluster_id:14841639068965178418 member_id:10276657743932975437 revision:55 raft_term:4监听到del：aaa 其实使用起来还是非常简单，我就不过多赘述了。]]></content>
      <categories>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入深出ETCD之【raft原理】]]></title>
    <url>%2F2019%2F06%2F12%2Fgolang%2Fopen-source-component%2Fetcd-raft%2F</url>
    <content type="text"><![CDATA[这次我们来说说，有关于etcd原理的一些事情。之前我们已经了解到了etcd是一个分布式的k-v存储，那么它究竟是如何保证数据是如何复制到每个节点上面去的呢？又是如何保证在网络分区的情况下能正常工作下去？raft协议到底是什么？带着这些问题我们继续往下看。 raft选举策略我们知道etcd使用raft协议来保证整个分布式的节点网络能正常的运转并且能正确的将数据复制到每个节点上面去。那么什么是raft协议嘞？ 首先我们有这样一个背景：raft是想维护整一个网络，其中有一个领导人，这个领导人负责将收到的信息同步给网络中的其他所有节点，从而保证整个网络数据一致。 如果你有一定的英文基础，我建议直接查看下面这个网站，它用动画非常清楚的描述了raft选举的整个过程：http://thesecretlivesofdata.com/raft/ 这个其实已经说明的超级棒了，如果你还看不懂，我下面会用最简单的几个要点来进行最简单的说明。 大多数理论首先说明一个理论，叫做大多数理论，很简单，举个栗子： 有10个人，如果你将苹果给其中的6个人（大多数），那么你随机选择5个人，一定有一个人会有苹果。 在etcd中的应用： 选举中只要有大多数（超过半数的人给你投票）你肯定就是票数最多的了，不可能有人比你更多。 只需要将日志复制给大多数的节点，那么只要有一半的节点正常工作就能保证数据最新 选举状态下面是一些选举过程中节点的状态leader 表示选举最终产生的领导人candidate 候选状态，表示当前正在参与选举follower 表示选举最终自己不是领导人，那自己就是从属节点 选举过程与要点 所有节点一开始都是follower状态 当节点处于follower状态时，每个节点随机经过一段时间，如果没有收到leader的消息就会进入candidate状态（证明当前没有leader节点需要重新进行选举），如果收到信息就会继续保持follower状态 当节点处于candidate就会要求别人给自己投票，收到大多数的节点的投票那就转变为leader状态，否则要么是别的节点成为了leader，要么就是因为特殊情况导致这次选举失败重新进行选举 每次选举举办的时候有一个term，在每一个term中，每个节点只能投票一次 投票的时候必须投给当前数据至少和自己一样的节点，并且term大的优先 日志复制规则etcd是通过日志复制来实现数据同步的这个图网上也很多，说明的是日志复制的规则每个节点都有一份自己的日志，有的节点多，有的节点少，日志最多的肯定是leader。上图还有几个要点，我看别人没提到，我就提一下： 颜色代表term 第四行表示的这个节点，第一term下复制了两个日志就异常挂掉了 最终只有第三行这个follower和第一行的leader保持了同步 异常情况raft之所以厉害因为即使出现一些特殊情况，整个网络在一定的时间之后也能自动恢复并正常工作。 一个节点的异常首先最常见的情况就是一个节点出现异常，有可能是这个节点的服务器挂了，或者别的什么原因。 如果出现问题的这个节点是follower，那么没有关系，整个网络依旧能正常运行，当这个节点再次加入网络的时候也只需要同步后面的数据即可。 如果出现问题的是leader，有一点麻烦，因为网络中没有leader节点了，那么就会重新进行选举，重新找一个leader，当这个异常节点恢复之后发现当前网络中有leader了，而且term还比自己大，那么自己就退位称为follower。 网络分区还有一种异常情况是由于网络导致的，网络出现异常，导致节点之间的通信存在异常，一部分节点与另一部分之间没有办法访问了。如下图所示： 上面三个follower没有办法与下面的节点进行通信。 当客户端再次请求leader发送数据的时候，leader发现没有办法将数据同步给给大多数节点，它只能给自己和旁边的一个，此时leader没有办法给客户端反馈。 上面三个节点由于收不到leader的消息，那么会认为网络中没有leader存在，会重新进行选举操作，因为当前上面有三个节点存在（只要有超过半数的节点参与选举就行），所以可以重新选举成功，选出新的leader告诉客户端，客户端就会重新发送数据到新的leader。 当网络恢复之后又会找到最新的leader从而将数据同步至最新的状态。 总结总的来说，只要整个网络中存在大多数节点正常运行，那么etcd就是可用的，并且能够保证数据正确。当网络恢复之后也能将数据调整到最新的状态。raft强大的地方在于它能自动的进行状态的变化，自动进行选举，并且选举遵循一定的策略，进而保证整个网络的正常运转。同时保证数据的一致性。了解etcd的这个原理有助于我们后续的使用以及源码的阅读。]]></content>
      <categories>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入深出ETCD之【简介与命令行使用】]]></title>
    <url>%2F2019%2F06%2F10%2Fgolang%2Fopen-source-component%2Fetcd-brief%2F</url>
    <content type="text"><![CDATA[你知道etcd吗？随着k8s的使用广泛之后，etcd被非常多的人所知道，同时又因为它可靠的分布式特性被很多人喜欢。所以，我准备有几篇博文来记录一下，从基本使用到线上部署再到原理分析，做一个系列。那么，今天先来说说它的简介与命令行的使用。 简介ETCD是什么我个人总结为下面用几个要点： 高可用K-V存储，就类似于redis一样的键值对存储。 允许应用实时监听存储中的K-V变化。 能够容忍单点故障，能够应对网络分区。 etcd利用raft在集群中同步K-V信息，raft是强一致的集群日志同步算法。 总结：etcd是一个分布式高可用k-v存储，通过复制达到每个节点存储的信息一致，从而保证高可用。 数据复制这里简单说一下复制的具体流程： （client为我们的客户端，用来发出存储请求，leader和follower都是etcd的节点）就如图上所看到的，我叫它两段式提交： 客户端请求leader发送存储的数据，然后leader节点要将信息通过日志复制给大多数的follower节点，如上图所示，只需要复制给两个（加上它自己是三个）那么就是大多数节点。 leader当复制完成之后才会本地提交，然后返回给客户端成功，（如果没有或者不能复制给大多数节点，那么则存储失败）此时再同时其他follower去他们自己本地提交。是不是有一种分布式事务的感觉？分布式的解决通常都是这种感觉。 我们也可以看到，etcd通过先将数据存放在大多数节点上面从而保证数据不会出错并且效率较高，最终所有节点数据还是会同步一致的。 官方给出写入的性能：1000/s PS:这里因为是简介，所以就简单提一下，有关如何选举出leader还有raft协议的一些具体细节，以及当出现网络分区或者节点异常问题的恢复会在之后的博客中给出。 存储结构底层存储key是有序排列的‘key’ -&gt; ‘value’ aaa/bbb -&gt; 111aaa/bbc -&gt; 3333bbb/aaa -&gt; 1321ccc -&gt; 24就是按照key的顺序依次排列，相同前缀的key会被放在一起，这样到存储结构，当查询时可以通过key的前缀将一系列的value都取出来 watch机制和lease租约etcd有一个很棒的机制要单独提一句，就是watch，它允许你去监控一个key的变化。当你监控了之后，这个key的添加修改删除都会被监控到。lease租约，这个机制和redis中的key过期机制一样，可以申请一个租约，这个租约有一个时间限制，比如60秒，你可以将这个租约设置到一个key上，那么这个key过60秒就会被自动删除。当然也可以进行续租。 具体使用情况，可以从后面的命令行操作中看到。 还有一些小点 etcd使用grpc，所以网络性能会高 部署节点数量要求是2N+1个 选举leader需要半数以上的节点参与 etcd是支持事务操作的，可以if第一次a提交正常，then提交b，else不提交b 本地单节点部署我们一开始学习和测试的时候只需要在本地部署一个单节点就可以了，单节点的部署比较方便这边简单说明一下。首先下载对应的版本：https://github.com/etcd-io/etcd/releases我这边使用的mac对应的darwin-amd64的版本，其他版本应该类似。下载解压之后有两个文件比较重要： etcd 这个是节点 etcdctl 这个是客户端进入所在目录使用命令进行启动和使用 使用节点命令1➜ ./etcd 使用客户端命令1234567➜ ./etcdctlNAME: etcdctl - A simple command line client for etcd.WARNING: Environment variable ETCDCTL_API is not set; defaults to etcdctl v2. Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API. 之后会出现上述类似警告，告诉你，默认使用的是v2版本的API，你需要设置环境变量ETCDCTL_API=3就能使用v3版本的API了，这里我们使用命令export ETCDCTL_API=3 或者你可以手动修改环境变量添加export ETCDCTL_API=3就可以了，当不出现警告的时候证明环境变量设置正确。 简单命令行操作下面介绍几个最基本的etcd的操作，其实非常简单。主要与redis不同的是拥有独特的watch机制，这个机制非常棒。 put1234➜ ./etcdctl put /aaa/a 1OK➜ ./etcdctl put /aaa/b 2OK get12345678➜ ./etcdctl get /aaa/a/aaa/a1➜ ./etcdctl get --prefix /aaa/aaa/a1/aaa/b2 –prefix意思是取出所有前缀为/aaa的key watch新开一个窗口使用命令watch进行监听1➜ ./etcdctl watch /aaa/a 然后对/aaa/a这个key的操作全部都会被监听到1234➜ ./etcdctl put /aaa/a 123OK➜ ./etcdctl del /aaa/a1 123456➜ ./etcdctl watch /aaa/aPUT/aaa/a123DELETE/aaa/a lease创建一个60s的租约12➜ ./etcdctl lease grant 60lease 694d6b2b7d7e6a0c granted with TTL(60s) 12➜ ./etcdctl put /aaa/a 123 --lease=694d6b2b7d7e6a0cOK put的时候使用租约注意，这里需要输入上面租约的16进制标识符然后监听的地方会发现，60秒后，/aaa/a这个key被自动删除了 当然你可以使用keep-alive进行续租，如：1➜ ./etcdctl lease keep-alive 694d6b2ac4a35625 总结以上简单说明了etcd的一些基本信息，单节点部署，以及一些基本用法，从上述信息我们总结可知： etcd是分布式的，能保证在单点故障下也能正常使用 分布式也会导致问题，etcd写入性能相较redis肯定有所不及 etcd独特的watch机制可以用于很多场景，如配置更新分发等 那这里就说这么多，看完你就应该大致知道etcd是个啥玩意了，从现在看来你可能还没有感觉它有什么厉害的地方，后面我们结合实际的场景使用就能更加明白了。]]></content>
      <categories>
        <category>etcd</category>
      </categories>
      <tags>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang指针与unsafe]]></title>
    <url>%2F2019%2F06%2F06%2Fgolang%2Fsource-code%2Fpoint-unsafe%2F</url>
    <content type="text"><![CDATA[我们知道在golang中是存在指针这个概念的。对于指针很多人有点忌惮（可能是因为之前学习过C语言），因为它会导致很多异常的问题。但是很多人学习之后发现，golang中的指针很简单，没有C那么复杂。所以今天就详细来说说指针。 指针的使用123a := 1p := &amp;afmt.Println(p) 输出：0xc42001c070 可以看到p就是一个指针，也可以说是a的地址。 1234a := 1var p *intp = &amp;afmt.Println(p) 或者也可以写成这样，因为我知道，在很多人看来，看到*号才是指针（手动滑稽） 123a := 1p := &amp;afmt.Println(*p) 输出：1 然后使用就直接通过*号就能去到对应的值了，就这么简单 指针的限制Golang中指针之所以看起来很简单，是因为指针的功能不多。我们能看到的功能就是指针的指向一个地址而已，然后对于这个地址也只能进行传递，或者通过这个的地址去访问值。 不能像C语言中一样p++，这样移动操作指针，因为其实这样操作确实不安全，很容易访问到奇怪的区域。 不同类型的指针不能相互赋值、转换、比较。会出现cannot use &amp;a (type int) as type float32 in assignment类似这样的错误 如果只是单纯说go中指针的功能，上面就已经说完了，没必要写博客，但是其实go中还有一个包叫unsafe，有了它，指针就可以像C一样想干嘛干嘛了。 unsafe三个类型其实指针有三种：一种是我们常见的*，用*去表示的指针；一种是unsafe.Pointer，Pointer是unsafe包下的一个类型；最后一种是uintptr，uintptr就厉害了，这玩意是可以进行运算的也就是可以++–； 他们之间有这样的转换关系：* &lt;=&gt; unsafe.Pointer &lt;=&gt; uintptr 有一点要注意的是，uintptr 并没有指针的语义，意思就是 uintptr 所指向的对象会被 gc 无情地回收。而 unsafe.Pointer 有指针语义，可以保护它所指向的对象在“有用”的时候不会被垃圾回收。 从这样的关系你大概就可以猜到，我们使用的指针*p转换成Pointer然后转换uintptr进行运算之后再原路返回，理论上就能等同于进行了指针的运算。我们下面就来实践一下。 unsafe操作slice12345678910111213func main() &#123; s := make([]int, 10) s[1] = 2 p := &amp;s[0] fmt.Println(*p) up := uintptr(unsafe.Pointer(p)) up += unsafe.Sizeof(int(0)) // 这里可不是up++哦 p2 := (*int)(unsafe.Pointer(up)) fmt.Println(*p2)&#125; 输出：02 从代码中我们可以看到，我们首先将指针指向切片的第一个位置，然后通过转换得到uintptr，操作uintptr + 上8位（注意这里不能++因为存放的是int，下一个元素位置相隔举例int个字节），最后转换回来得到指针，取值，就能取到切片的第二个位置了。 unsafe操作struct当然有人肯定要说了，上面那个一顿操作猛如虎，不就是访问下一个位置嘛，我直接访问就行了。那下面就是厉害的来了，我们知道如果一个结构体里面定义的属性是私有的，那么这个属性是不能被外界访问到的。我们来看看下面这个操作： 123456package basictype User struct &#123; age int name string&#125; 12345678910111213141516package mainfunc main() &#123; user := &amp;basic.User&#123;&#125; fmt.Println(user) s := (*int)(unsafe.Pointer(user)) *s = 10 up := uintptr(unsafe.Pointer(user)) + unsafe.Sizeof(int(0)) namep := (*string)(unsafe.Pointer(up)) *namep = "xxx" fmt.Println(user)&#125; User是另外一个basic包中的结构体，其中的age是小写开头的，理论上来说，我们在外部没有办法修改age的值，但是经过上面这波操作之后，输出信息是：&amp;{0 }&amp;{10 xxx}也就是说成功操作到了结构体的私有属性。 顺便提一句：创建结构体会被分配一块连续的内存，结构体的地址也代表了第一个成员的地址。 下面我们来验证一下你是否已经学会了unsafe的操作，尝试不看一个小结，自己尝试一下：如何完成字符串到[]byte的转换，并且不开辟新的空间？ 字符串和byte数组转换inplace我们知道如果将字符串转换成[]byte非常方便12s := "123"a := []byte(s) 但是这样需要开辟额外的空间，那么如何实现原地的，不需要拷贝数据的转换呢？我们想一下，其实从底层的存储角度来说，string的存储规则和[]byte是一样的，也就是说，其实指针都是从某个位置开始到一段空间，中间一格一格。所以利用unsafe就可以做到。 123456789101112func main() &#123; s := "123" a := []byte(s) print("s = " , &amp;s, "\n") print("a = " , &amp;a, "\n") a2 := (*[]byte)(unsafe.Pointer(&amp;s)) print("a2 = " , a2, "\n") fmt.Println(*a2)&#125; 输出结果：s = 0xc420055f40a = 0xc420055f60a2 = 0xc420055f40[49 50 51] 我们可以看到s和a的地址是不一样的，但是s和a2的地址是一样的，并且a2已经是一个[]byte了。嘿嘿嘿~你以为这样就结束了？？？ 存在的问题其实这个转换是存在问题的，问题就在新的[]byte的Cap没有正确的初始化。我们打印一下cap看一下fmt.Println(“cap a =”, cap(a))fmt.Println(“cap a2 =”, cap(*a2))结果是：cap a = 32cap a2 = 17418400这么大的容量是要上天呢？？？ 问题的原因在src/reflect/value.go下看12345678910type StringHeader struct &#123; Data uintptr Len int&#125;type SliceHeader struct &#123; Data uintptr Len int Cap int&#125; 看到其实string没有cap而[]byte有，所以导致问题出现，也容易理解，string是没有容量扩容这个说法的，所以新的[]byte没有赋值cap所以使用了默认值。 问题解决123456789stringHeader := (*reflect.StringHeader)(unsafe.Pointer(&amp;s))bh := reflect.SliceHeader&#123; Data: stringHeader.Data, Len: stringHeader.Len, Cap: stringHeader.Len,&#125;return *(*[]byte)(unsafe.Pointer(&amp;bh)) 通过重新设置SliceHeader就可以完成 总结以上就是所有golang指针和unsafe的相关细节和使用。那么肯定有人会问这个有什么用了？ 1、没啥事你就别乱用了，别人都说unsafe不安全了。 2、源码中很多大量的使用了指针移动的操作。 如map中通过key获取value的时候： v := add(unsafe.Pointer(b), dataOffset+bucketCnt uintptr(t.keysize)+i uintptr(t.valuesize)) 通过桶的指针的偏移拿到值，具体我就不多介绍了。总之对于你看golang源码的时候会有很大帮助的。可能必要的时候你也能用到它，还是那句话，除非你知道它在干什么，否则不要用。]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>unsafe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话图解golang map源码详解]]></title>
    <url>%2F2019%2F06%2F03%2Fgolang%2Fsource-code%2Fgraphic-golang-map%2F</url>
    <content type="text"><![CDATA[网上分析golang中map的源码的博客已经非常多了，随便一搜就有，而且也非常详细，所以如果我再来写就有点画蛇添足了（而且我也写不好，手动滑稽）。但是我还是要写，略略略，这篇博客的意义在于能从几张图片，然后用我最通俗的文字，让没看过源码的人最快程度上了解golang中map是怎么样的。 当然，因为简单，所以不完美。有很多地方省略了细节问题，如果你觉得没看够，或者本来就想了解详细情况的话在文末给出了一些非常不错的博客，当然有能力还是自己去阅读源码比较靠谱。 那么下面我将从这几个方面来说明，你先记住有下面几个方向，这样可以有一个大致的思路： 基础结构：golang中的map是什么样子的，是由什么数据结构组成的？ 初始化：初始化之后map是怎么样的？ get：如何获取一个元素？ put：如何存放一个元素？ 扩容：当存放空间不够的时候扩容是怎么扩的？ 基础结构图解这个就是golang中map的结构，其实真的不复杂，我省略了其中一些和结构关系不大的字段，就只剩下这些了。 大话大话来描述一些要点： 最外面是hmap结构体，用buckets存放一些名字叫bmap的桶（数量不定，是2的指数倍） bmap是一种有8个格子的桶（一定只有8个格子），每个格子存放一对key-value bmap有一个overflow，用于连接下一个bmap（溢出桶） hmap还有oldbuckets，用于存放老数据（用于扩容时） mapextra用于存放非指针数据（用于优化存储和访问），内部的overflow和oldoverflow实际还是bmap的数组。 这就是map的结构，然后我们稍微对比总结一下。 我们常见的map如java中的map是直接拿数组，数组中直接对应出了key-value，而在golang中，做了多加中间一层，buckets；java中如果key的哈希相同会采用链表的方式连接下去，当达到一定程度会转换红黑树，golang中直接类似链表连接下去，只不过连接下去的是buckets。 源码一瞥 下面附上源码中它们的样子，方便之后你自己阅读的时候有个印象（注意源码中的样子和编译之后是不同的哟，golang会根据map存放的类型不同来搞定它们实际的样子） 那么看完结构你肯定会有疑问？为什么要多一层8个格子的bucket呢？我们怎么确定放在8个格子其中的哪个呢？带着问题往下看。 初始化源码一瞥初始化就不需要图去说明了，因为初始化之后就是产生基础的一个结构，根据map中存放的类型不同。这里主要说明一下，初始化的代码放在什么位置。我也删除了其中一些代码，大致看看就好。 123456789101112131415161718192021222324252627282930313233// makehmap_small implements Go map creation for make(map[k]v) and// make(map[k]v, hint) when hint is known to be at most bucketCnt// at compile time and the map needs to be allocated on the heap.func makemap_small() *hmap &#123; h := new(hmap) h.hash0 = fastrand() return h&#125;// makemap implements Go map creation for make(map[k]v, hint).// If the compiler has determined that the map or the first bucket// can be created on the stack, h and/or bucket may be non-nil.// If h != nil, the map can be created directly in h.// If h.buckets != nil, bucket pointed to can be used as the first bucket.func makemap(t *maptype, hint int, h *hmap) *hmap &#123; ..... // initialize Hmap if h == nil &#123; h = (*hmap)(newobject(t.hmap)) &#125; h.hash0 = fastrand() // find size parameter which will hold the requested # of elements B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B ...... return h&#125; 其中需要注意一个点：“B”，还记得刚才说名字叫bmap的桶数量是不确定的吗？这个B一定程度上表示的就是桶的数量，当然不是说B是3桶的数量就是3，而是2的3次方，也就是8；当B为5，桶的数量就是32；记住这个B，后面会用到它。 其实你想嘛，初始化还能干什么，最重要的肯定就是确定一开始要有多少个桶，初始的大小还是很重要的，还有一些别的初始化哈希种子等等，问题不大。我们的重点还是要放在存/取上面。 GET图解其实从结构上面来看，我们已经可以摸到一些门道了。先自己想一下，要从一个hashmap中获取一个元素，那么一定是通过key的哈希值去定位到这个元素，那么想着这个大致方向，看下面一张流程图来详细理解golang中是如何实现的。 大话下面说明要点： 计算出key的hash 用最后的“B”位来确定在哪个桶（“B”就是前面说的那个，B为4，就有16个桶，0101用十进制表示为5，所以在5号桶） 根据key的前8位快速确定是在哪个格子（额外说明一下，在bmap中存放了每个key对应的tophash，是key的前8位） 最终还是需要比对key完整的hash是否匹配，如果匹配则获取对应value 如果都没有找到，就去下一个overflow找 总结一下：通过后B位确定桶，通过前8位确定格子，循环遍历连着的所有桶全部找完为止。那么为什么要有这个tophash呢？因为tophash可以快速确定key是否正确，你可以把它理解成一种缓存措施，如果前8位都不对了，后面就没有必要比较了。 源码一瞥其中红色的字标出的地方说明了上面的关键点，最后有关key和value具体的存放方式和取出的定位不做深究，有兴趣可以看最后的参考博客。 PUT其实当你知道了如何GET，那么PUT就没有什么难度了，因为本质是一样的。PUT的时候一样的方式去定位key的位置： 通过key的后“B”位确定是哪一个桶 通过key的前8位快速确定是否已经存在 最终确定存放位置，如果8个格子已经满了，没地方放了，那么就重新创建一个bmap作为溢出桶连接在overflow 图解这里主要图解说明一下，如果新来的key发现前面有一个格子空着（这个情况是删除造成的），就会记录这个位置，当全部扫描完成之后发现自己确实是新来的，那么就会放前面那个空着的，而不会放最后（我把这个称为紧凑原则，尽可能保证数据存放紧凑，这样下次扫描会快） 代码位置go/src/runtime/hashmap.go的mapassign函数就是map的put方法，因为代码很长这里就不多赘述了。 扩容这个就是最复杂的地方了，但是呢？Don’t worry我这里还是会省略其中某些部分，将最重要的地方拎出来。 扩容的方式 相同容量扩容 2倍容量扩容啥意思呢？第一种出现的情况是：因为map不断的put和delete，出现了很多空格，这些空格会导致bmap很长，但是中间有很多空的地方，扫描时间变长。所以第一种扩容实际是一种整理，将数据整理到前面一起。第二种呢：就是真的不够用了，扩容两倍。 扩容的条件装载因子如果你看过Java的HashMap实现，就知道有个装载因子，同样的在golang中也有，但是不一样哦。装载因子的定义是这个样子：loadFactor := count / (2^B)其中count为map中元素的个数，B就是之前个那个“B”翻译一下就是装载因子 = （map中元素的个数）/（map当前桶的个数） 扩容条件1装载因子 &gt; 6.5（这个值是源码中写的）其实意思就是，桶只有那么几个，但是元素很多，证明有很多溢出桶的存在（可以想成链表拉的太长了），那么扫描速度会很慢，就要扩容。 扩容条件2overflow 的 bucket 数量过多：当 B 小于 15，如果 overflow 的 bucket 数量超过 2^B ；当 B &gt;= 15，如果 overflow 的 bucket 数量超过 2^15 。其实意思就是，可能有一个单独的一条链拉的很长，溢出桶太多了，说白了就是，加入的key不巧，后B位都一样，一直落在同一个桶里面，这个桶一直放，虽然装载因子不高，但是扫描速度就很慢。 扩容条件3当前不能正在扩容 图解这张图表示的就是相同容量的扩容，实际上就是一种整理，将分散的数据集合到一起，提高扫描效率。（上面表示扩容之前，下面表示扩容之后） 这张图表示的是就是2倍的扩容（上面表示扩容之前，下面表示扩容之后），如果有两个key后三位分别是001和101，当B=2时，只有4个桶，只看最后两位，这两个key后两位都是01所以在一个桶里面；扩容之后B=3，就会有8个桶，看后面三位，于是它们就分到了不同的桶里面。 大话下面说一些扩容时的细节： 扩容不是一次性完成的，还记的我们hmap一开始有一个oldbuckets吗？是先将老数据存到这个里面 每次搬运1到2个bucket，当插入或修改、删除key触发 扩容之后肯定会影响到get和put，遍历的时候肯定会先从oldbuckets拿，put肯定也要考虑是否要放到新产生的桶里面去 源码一瞥扩容的三个条件，看到了吗？这个地方在mapassign方法中。 这里可以看到，注释也写的很清楚，如果是加载因子超出了，那么就2倍扩容，如果不是那么就是因为太多溢出桶了，sameSizeGrow表示就是相同容量扩容 evacuate是搬运方法，这边可以看到，每次搬运是1到2个 evacuate实在是太长了，也非常复杂，但是情况就是图上描述的那样，有兴趣的可以详细去看，这里不截图说明了。 总结和小问题至此你应该对于golang中的map有一个基本的认识了，你还可以去看看删除，你还可以去看看遍历等等，相信有了上面的基本认识那么应该不会难到你。下面有几个小问题： 是否线程安全？否，而且并发操作会抛出异常。 源码位置：src/runtime/hashmap.go 每次遍历map顺序是否一致？不一致，每次遍历会随机个数，通过随机数来决定从哪个元素开始。 写的仓促，难免疏漏，有问题的地方还请批评指正。 参考资料如果你希望看到源码的各种细节讲解，下面这几篇是我学习的时候看的，供你参考，希望对你有帮助https://github.com/qcrao/Go-Questions/tree/master/maphttps://github.com/cch123/golang-notes/blob/master/map.mdhttps://draveness.me/golang-hashmaphttps://lukechampine.com/hackmap.html]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang 读写锁RWMutex 互斥锁Mutex 源码详解]]></title>
    <url>%2F2019%2F06%2F01%2Fgolang%2Fsource-code%2Frwmutex-mutex-source-code-review%2F</url>
    <content type="text"><![CDATA[Golang中有两种类型的锁，Mutex （互斥锁）和RWMutex（读写锁）对于这两种锁的使用这里就不多说了，本文主要侧重于从源码的角度分析这两种锁的具体实现。 引子问题我一般喜欢带着问题去看源码。那么对于读写锁，你是否有这样的问题，为什么可以有多个读锁？有没有可能出现有协程一直无法获取到写锁的情况？带着你的疑问来往下看看，具体这个锁是如何实现的。 如果你自己想看，我给出阅读的一个思路，可以先看读写锁，因为读写锁的实现依赖于互斥锁，并且读写锁比较简单一些，然后整理思路之后再去想一下实际的应用场景，然后再去看互斥锁。 下面我就会按照这个思路一步步往下走。 基础知识点 知识点1：信号量信号量是 Edsger Dijkstra 发明的数据结构（没错就是那个最短路径算法那个牛人），在解决多种同步问题时很有用。其本质是一个整数，并关联两个操作： 申请acquire（也称为 wait、decrement 或 P 操作）释放release（也称 signal、increment 或 V 操作） acquire操作将信号量减 1，如果结果值为负则线程阻塞，且直到其他线程进行了信号量累加为正数才能恢复。如结果为正数，线程则继续执行。release操作将信号量加 1，如存在被阻塞的线程，此时他们中的一个线程将解除阻塞。 知识点2：锁的定义在goalng中如果实现了Lock和Unlock方法，那么它就可以被称为锁。 知识点3：锁的自旋：（详见百度） 知识点4：cas算法：（最好有所了解，不知道问题也不大） 读写锁RWMutex首先我们来看看RWMutex大体结构看到结构发现读写锁内部包含了一个w Mutex互斥锁注释也很明确，这个锁的目的就是控制多个写入操作的并发执行writerSem是写入操作的信号量readerSem是读操作的信号量readerCount是当前读操作的个数readerWait当前写入操作需要等待读操作解锁的个数这几个现在看不懂没关系，后面等用到了你再回来看就好了。 然后我们看看方法一共有5个方法，看起来就不复杂，我们一个个来看。 这个最简单，就是返回一个locker对象没啥好说的 问题的关键就在于锁和解锁的几个方法，因为我已经看过，所以推荐这几个方法的阅读顺序是RLock Lock RUnlock Unlock RLock（获取读锁）先不看竞态检测的部分，先重点看红色框中的部分可以看到，其实很简单，每当有协程需要获取读锁的时候，就将readerCount + 1但是需要注意的是，这里有一个条件，当readerCount + 1之后的值 &lt; 0的时候，那么将会调用runtime_Semacquire方法这个方法是一个runtime的方法，会一直等待传入的s出现&gt;0的时候然后我们可以记得，这里有这样一个情况，当出先readerCount + 1为负数的情况那么就会被等待，看注释我们可以猜到，是当有写入操作出现的时候，那么读操作就会被等待。 Lock（获取写锁）写锁稍微复杂一些，但是样子也差不多，我们还是先来看红色框中的部分。首先操作最前面说的互斥锁，目的就是处理多个写锁并发的情况，因为我们知道写锁只有一把。这里不需要深入互斥锁，只需要知道，互斥锁只有一个人能拿到，所以写锁只有一个人能拿到。 然后重点来了，这里的这个操作细细体会一下，atomic.AddInt32(&amp;rw.readerCount, -rwmutexMaxReaders)是将当前的readerCount减去一个非常大的值rwmutexMaxReaders为1 &lt;&lt; 30大概是1073741823这么大吧 所以我们可以从源码中看出，readerCount由于每有一个协程获取读锁就+1，一直都是正数，而当有写锁过来的时候，就瞬间减为很大的负数。然后做完上面的操作以后的r其实就是原来的readerCount。后面进行判断，如果原来的readerCount不为0（原来有协程已经获取到了读锁）并且将readerWait加上readerCount（表示需要等待readerCount这么多个读锁进行解锁），如果满足上述条件证明原来有读锁，所以暂时没有办法获取到写锁，所以调用runtime_Semacquire进行等待，等待的信号量为writerSem RUnlock（释放读锁）如果是我们来写的话，可能就是将之前+1的readerCount，-1就完事了，但是其实还有一些操作需要注意。如果-1之后+1==0是啥情况？没错就是我们常见的，新手程序员，没有获取读锁就想去释放读锁，于是异常了。当然+1之后刚好是rwmutexMaxReaders，就证获取了写锁而去释放了读锁，导致异常。除去异常情况，剩下的就是r还是&lt;0的情况，那么证明确实有协程正在想要获取写锁，那么就需要操作我们前面看到的readerWait，当readerWait减到0的时候就证明没有人正在持有写锁了，就通过信号量writerSem的变化告知刚才等待的协程（想要获取写锁的协程）：你可以进行获取了。 到这里你可以把思路大致串起来了，然后懂了再往下看。 Unlock（释放写锁）写锁释放需要恢复readerCount，还记得上锁的时候减了一个很大的数，这个时候要加回来了。当然加完之后如果&gt;=rwmutexMaxReaders本身，那么还是新手程序员的问题，当没有获取写锁的时候就开始想着释放写锁了。然后for循环就是为了通知所有在我们RLock方法中看到的，当有因为持有写锁所以等待的那些协程，通过信号量readerSem告诉他们可以动了。最后别忘记还有一个互斥锁需要释放，让别的协程也可以开始抢写锁了。 至此，读写锁的分析基本上告一段落了。针对于其中关于竞态分析的代码，有兴趣的小伙伴可以去了解一下。 互斥锁Mutex互斥锁比读写锁复杂，但是好在golang给的注释很详细，所以也不困难（注释真的很重要）。我们先来看看里面的一段注释：很长的一段英文，我用英语四级的翻译能力给你翻译一下，可以将就看看，如果可以建议你仔细看英文看懂它，因为这对于后面的源码阅读非常重要。///这个互斥锁是公平锁 互斥锁有两种操作模式：正常模式和饥饿模式。在正常模式下等待获取锁的goroutine会以一个先进先出的方式进行排队，但是被唤醒的等待者并不能代表它已经拥有了这个mutex锁，它需要与新到达的goroutine争夺mutex锁。新来的goroutine有一个优势 —— 他们已经在CPU上运行了并且他们，所以抢到的可能性大一些，所以一个被唤醒的等待者有很大可能抢不过。在这样的情况下，被唤醒的等待者在队列的头部。如果一个等待者抢锁超过1ms失败了，就会切换为饥饿模式。 在饥饿模式下，mutex锁会直接由解锁的goroutine交给队列头部的等待者。新来的goroutine不能尝试去获取锁，即使可能根本就没goroutine在持有锁，并且不能尝试自旋。取而代之的是他们只能排到队伍尾巴上乖乖等着。 如果一个等待者获取到了锁，并且遇到了下面两种情况之一，就恢复成正常工作模式。情况1：它是最后一个队列中的等待者。情况2：它等待的时间小于1ms 正常模式下，即使有很多阻塞的等待者，有更好的表现，因为一轮能多次获得锁的机会。饥饿模式是为了避免那些一直在队尾的倒霉蛋。/// 我的话简单总结就是，互斥锁有两种工作模式，竞争模式和队列模式，竞争就是大家一起抢，队列就是老老实实排队，这两种工作模式会通过一些情况进行切换。 首先还是来看看大体结构可以看到，相对读写锁，结构上面很简单，只有两个值，但是千万不要小瞧它，减少了字段就增加了理解难度。state：将一个32位整数拆分为：当前阻塞的goroutine数(29位)饥饿状态(1位，0为正常模式；1为饥饿模式)唤醒状态(1位，0未唤醒；1已唤醒)锁状态(1位，0可用；1占用) sema：信号量 方法也很简单，就是Lock和Unlock两个方法，一个上锁，一个解锁，没啥好说的。 一个方法我们先来看一个的要用到的方法 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)这个函数，会先判断参数addr指向的被操作值与参数old的值是否相等，如果相等会将参数new替换参数addr所指向的值，不然的话就啥也不做。需要特别说明的是，这个方法并不会阻塞。 几个常量这是定义的几个常量，我们在一开始的注释周围可以看到，后面需要用到，暂时记住它们的初始值就好。 mutexLocked = 1 &lt;&lt; iota // 1左移0位，是1，二进制是1，（1表示已经上锁）mutexWoken // 1左移1位，是2，二进制是10mutexStarving // 1左移2位，是4，二进制是100mutexWaiterShift = iota // 就是3， 二进制是11 starvationThresholdNs = 1e6 // 这个就是我们一开始在注释里面看到的1ms，一定超过这个门限值就会更换模式 Lock获取锁因为Lock方法比较长，所以我切分一段段看，需要完整的请自己翻看源码。要注意的一点是，一定要时刻记住，Lock方法是做什么的，很简单，就是要抢锁。看不懂的时候想想这个目标。第一步，判断state状态是否为0，如果为0，证明没有协程持有锁，那么就很简单了，直接获取到锁，将mutexLocked（为1）赋值到state就可以了。 看后面的方法时，告诉需要告诉你们一个小技巧，当遇到这种位操作很多的情况，有两个方法挺好用，对于你看源码会有帮助：第一个是将所有定值先计算，然后判断非定值的情况；第二个是将所有的计算写下来，自己用笔去计算，不要执着于打字。 然后我们以下面这个段举例：首先，看注释应该能明白这一段大致意思是，如果不是饥饿模式，就会进行自旋操作，然后不断循环。 然后根据上面的技巧，old&amp;(mutexLocked|mutexStarving) == mutexLocked（下面均为二进制）mutexLocked = 1mutexStarving = 11mutexLocked = 1这三个是定值，所以我们容易得到，满足情况的结果为，当old为xxxx0xx（二进制第三位为0）等式成立。也就是我们一开始说的，state的第三位是表示这个锁当前的模式，0为正常模式，1为饥饿模式。 那么第一个if就表示，如果当前模式为正常模式，且可以自旋，就进入if条件内部。if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 &amp;&amp; 同样的分析，awoke表示是否唤醒，old&amp;mutexWoken是取第二位，0表示当前协程未被唤醒，old&gt;&gt;mutexWaiterShift表示右移3位，也就是前29位，不为0证明有协程在等待，并且尝试去对比当前m.state与取出时的old状态，尝试去唤醒自己。然后自旋，并且增加自旋次数表示iter，然后重新赋值old。再循环下一次。 （你自己理一理，确实有点绕，仔细想想就想通了就对了。） 以上是使用自旋的情况，就是canSpin的。 然后进行判断old&amp;mutexStarving == 0就是第三位为0的情况，还是所说的正常模式。new就马上拿到锁了，new |= mutexLocked，表示或1，就是第一位无论是啥都赋值为1 old&amp;(mutexLocked|mutexStarving)，也就是old &amp; 0101必须当old的1和3两个位置为1的时候才是true，也就是说当前处于饥饿模式，并且锁已经被占用的情况，那么就需要排队去。排队也很精妙，new += 1 &lt;&lt; mutexWaiterShift这边注意是先计算1 &lt;&lt; mutexWaiterShift也就是将new的前29位+1，就是表示有一个协程在等待了。 好了到这里你的位操作应该就习惯的差不多了，之后我就直接说结论，不仔细的帮你01表示了，你已经长大了，要学会自己动手了。 如果当前已经标记为饥饿模式，并且没有锁住，那么设置new为饥饿模式if starving &amp;&amp; old&amp;mutexLocked != 0 { new |= mutexStarving} 如果唤醒，需要在两种情况下重设标志if awoke { 如果唤醒标志为与awoke不相协调就panic if new&amp;mutexWoken == 0 { throw(“sync: inconsistent mutex state”) } 设置唤醒状态位0,被唤醒 new &amp;^= mutexWoken} 如果获取锁成功 old&amp;(mutexLocked|mutexStarving) == 0成立表示已经获取锁，就直接退出CAS 中间这一段我就不多解释了，就是最前面注释说的，满足什么条件转换什么模式，不多说了。然后从队列中，也就是前29位-1。需要注意其中有一个runtime_SemacquireMutex和之前看的的runtime_Semacquire是一个意思，只是多了一个参数。这个就是这个方法的注释。可以看到，就是多了个队列去排队。 如果获取锁失败，old刷新状态再次循环，继续cas UnLock释放锁 Unlock就相对简单一些，竞态分析不看。其实我们自己想也能想到，unlock就是将标识位改回来嘛。然后因为我们已经看过读写锁了，也是同样的道理，如果没有上锁就直接解锁，那肯定报错嘛。 然后如果是正常模式，如果没有等待的goroutine或goroutine已经解锁完成的情况就直接返回了。如果有等待的goroutine那就通过信号量去唤醒runtime_Semrelease（注意这里是false），同时操作一下队列-1 如果是饥饿模式就直接唤醒（注意这里是true），反正有队列嘛。 互斥锁总结其实话说回来，我们其实看起来也简单，没有冲突的情况下，能拿就拿呗，如果出现冲突了就尝试自旋解决（自旋一般都能解决）如果解决不了就通过信号量解决，同时如果正常模式就是我们说的抢占式，非公平，如果是饥饿模式，就是我们说的排队，公平，防止有一些倒霉蛋一直抢不到。 整体总结一下，看完源码我们发现，其实锁的设计并不复杂，主要设计我们要学到cas和处理读写状态的信号量通知，对于那些位操作，能看懂，学可能一时半会学不会，因为很难在一开始就设计的那么巧妙，你也体会到了只用一个变量就维护了整个体系是一种艺术。]]></content>
      <categories>
        <category>golang源码解析</category>
      </categories>
      <tags>
        <tag>RWMutex</tag>
        <tag>Mutex</tag>
      </tags>
  </entry>
</search>
